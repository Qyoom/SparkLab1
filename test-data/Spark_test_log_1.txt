------ example.Helper.getSparkContext----------------
cores: 4
parallelism: 12
master: local[4]
JARS: List()
App name: Spark Template
SPARK_HOME: null
-----------------------------------------------------
14/05/14 14:54:55 INFO slf4j.Slf4jLogger: Slf4jLogger started
14/05/14 14:54:55 INFO Remoting: Starting remoting
14/05/14 14:54:55 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@10.65.10.255:60926]
14/05/14 14:54:55 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@10.65.10.255:60926]
14/05/14 14:54:55 INFO spark.SparkEnv: Registering BlockManagerMaster
14/05/14 14:54:56 INFO storage.DiskBlockManager: Created local directory at /var/folders/qk/q84p77h56y371pyw0vp69j1h0000gn/T/spark-local-20140514145456-162d
14/05/14 14:54:56 INFO storage.MemoryStore: MemoryStore started with capacity 74.4 MB.
14/05/14 14:54:56 INFO network.ConnectionManager: Bound socket to port 60927 with id = ConnectionManagerId(10.65.10.255,60927)
14/05/14 14:54:56 INFO storage.BlockManagerMaster: Trying to register BlockManager
14/05/14 14:54:56 INFO storage.BlockManagerMasterActor$BlockManagerInfo: Registering block manager 10.65.10.255:60927 with 74.4 MB RAM
14/05/14 14:54:56 INFO storage.BlockManagerMaster: Registered BlockManager
14/05/14 14:54:56 INFO spark.HttpServer: Starting HTTP Server
14/05/14 14:54:56 INFO server.Server: jetty-7.6.8.v20121106
14/05/14 14:54:56 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:60928
14/05/14 14:54:56 INFO broadcast.HttpBroadcast: Broadcast server started at http://10.65.10.255:60928
14/05/14 14:54:56 INFO spark.SparkEnv: Registering MapOutputTracker
14/05/14 14:54:56 INFO spark.HttpFileServer: HTTP File server directory is /var/folders/qk/q84p77h56y371pyw0vp69j1h0000gn/T/spark-265a90ed-d5a0-485d-94e5-e914ca9af6b6
14/05/14 14:54:56 INFO spark.HttpServer: Starting HTTP Server
14/05/14 14:54:56 INFO server.Server: jetty-7.6.8.v20121106
14/05/14 14:54:56 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:60929
14/05/14 14:54:56 INFO server.Server: jetty-7.6.8.v20121106
14/05/14 14:54:56 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/storage/rdd,null}
14/05/14 14:54:56 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/storage,null}
14/05/14 14:54:56 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages/stage,null}
14/05/14 14:54:56 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages/pool,null}
14/05/14 14:54:56 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages,null}
14/05/14 14:54:56 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/environment,null}
14/05/14 14:54:56 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/executors,null}
14/05/14 14:54:56 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/metrics/json,null}
14/05/14 14:54:56 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/static,null}
14/05/14 14:54:56 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/,null}
14/05/14 14:54:56 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
14/05/14 14:54:56 INFO ui.SparkUI: Started Spark Web UI at http://10.65.10.255:4040
2014-05-14 14:54:56.609 java[26460:1003] Unable to load realm info from SCDynamicStore
14/05/14 14:54:56 INFO storage.MemoryStore: ensureFreeSpace(35456) called with curMem=0, maxMem=77974732
14/05/14 14:54:56 INFO storage.MemoryStore: Block broadcast_0 stored as values to memory (estimated size 34.6 KB, free 74.3 MB)
-------------------------------------------------
>> data:
14/05/14 14:54:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/05/14 14:54:56 WARN snappy.LoadSnappy: Snappy native library not loaded
14/05/14 14:54:57 INFO mapred.FileInputFormat: Total input paths to process : 1
14/05/14 14:54:57 INFO spark.SparkContext: Starting job: take at BinaryClassification_tut1.scala:17
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Got job 0 (take at BinaryClassification_tut1.scala:17) with 1 output partitions (allowLocal=true)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Final stage: Stage 0 (take at BinaryClassification_tut1.scala:17)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Computing the requested partition locally
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:57 INFO spark.SparkContext: Job finished: take at BinaryClassification_tut1.scala:17, took 0.04344 s
1 0 2.52078447201548 0 0 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0
0 2.857738033247042 0 0 2.619965104088255 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0
0 2.857738033247042 0 2.061393766919624 0 0 2.004684436494304 0 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0
1 0 0 2.061393766919624 2.619965104088255 0 2.004684436494304 2.000347299268466 0 0 0 0 2.055002875864414 0 0 0 0
1 2.857738033247042 0 2.061393766919624 2.619965104088255 0 2.004684436494304 0 0 0 0 0 2.055002875864414 0 0 0 0
0 2.857738033247042 0 2.061393766919624 2.619965104088255 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0
1 0 0 0 2.619965104088255 0 2.004684436494304 0 0 2.228387042742021 2.228387042742023 0 2.055002875864414 0 0 0 0
1 0 0 0 2.619965104088255 0 2.004684436494304 0 0 2.228387042742021 2.228387042742023 0 2.055002875864414 0 0 0 0
0 2.857738033247042 0 2.061393766919624 2.619965104088255 0 2.004684436494304 2.000347299268466 2.122974378789621 2.228387042742021 2.228387042742023 0 0 0 0 12.72816758217773 0
0 2.857738033247042 0 0 2.619965104088255 0 0 0 0 2.228387042742021 2.228387042742023 0 2.055002875864414 0 0 0 0
-------------------------------------------------
>> parsedData:
14/05/14 14:54:57 INFO spark.SparkContext: Starting job: take at BinaryClassification_tut1.scala:24
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Got job 1 (take at BinaryClassification_tut1.scala:24) with 1 output partitions (allowLocal=true)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Final stage: Stage 1 (take at BinaryClassification_tut1.scala:24)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Computing the requested partition locally
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:57 INFO spark.SparkContext: Job finished: take at BinaryClassification_tut1.scala:24, took 0.009395 s
LabeledPoint(1.0, [0.0, 2.52078447201548, 0.0, 0.0, 0.0, 2.004684436494304, 2.000347299268466, 0.0, 2.228387042742021, 2.228387042742023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
LabeledPoint(0.0, [2.857738033247042, 0.0, 0.0, 2.619965104088255, 0.0, 2.004684436494304, 2.000347299268466, 0.0, 2.228387042742021, 2.228387042742023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
LabeledPoint(0.0, [2.857738033247042, 0.0, 2.061393766919624, 0.0, 0.0, 2.004684436494304, 0.0, 0.0, 2.228387042742021, 2.228387042742023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
LabeledPoint(1.0, [0.0, 0.0, 2.061393766919624, 2.619965104088255, 0.0, 2.004684436494304, 2.000347299268466, 0.0, 0.0, 0.0, 0.0, 2.055002875864414, 0.0, 0.0, 0.0, 0.0])
LabeledPoint(1.0, [2.857738033247042, 0.0, 2.061393766919624, 2.619965104088255, 0.0, 2.004684436494304, 0.0, 0.0, 0.0, 0.0, 0.0, 2.055002875864414, 0.0, 0.0, 0.0, 0.0])
LabeledPoint(0.0, [2.857738033247042, 0.0, 2.061393766919624, 2.619965104088255, 0.0, 2.004684436494304, 2.000347299268466, 0.0, 2.228387042742021, 2.228387042742023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
LabeledPoint(1.0, [0.0, 0.0, 0.0, 2.619965104088255, 0.0, 2.004684436494304, 0.0, 0.0, 2.228387042742021, 2.228387042742023, 0.0, 2.055002875864414, 0.0, 0.0, 0.0, 0.0])
LabeledPoint(1.0, [0.0, 0.0, 0.0, 2.619965104088255, 0.0, 2.004684436494304, 0.0, 0.0, 2.228387042742021, 2.228387042742023, 0.0, 2.055002875864414, 0.0, 0.0, 0.0, 0.0])
LabeledPoint(0.0, [2.857738033247042, 0.0, 2.061393766919624, 2.619965104088255, 0.0, 2.004684436494304, 2.000347299268466, 2.122974378789621, 2.228387042742021, 2.228387042742023, 0.0, 0.0, 0.0, 0.0, 12.72816758217773, 0.0])
LabeledPoint(0.0, [2.857738033247042, 0.0, 0.0, 2.619965104088255, 0.0, 0.0, 0.0, 0.0, 2.228387042742021, 2.228387042742023, 0.0, 2.055002875864414, 0.0, 0.0, 0.0, 0.0])
14/05/14 14:54:57 INFO spark.SparkContext: Starting job: first at GeneralizedLinearAlgorithm.scala:121
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Got job 2 (first at GeneralizedLinearAlgorithm.scala:121) with 1 output partitions (allowLocal=true)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Final stage: Stage 2 (first at GeneralizedLinearAlgorithm.scala:121)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Computing the requested partition locally
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:57 INFO spark.SparkContext: Job finished: first at GeneralizedLinearAlgorithm.scala:121, took 0.004408 s
14/05/14 14:54:57 INFO spark.SparkContext: Starting job: count at DataValidators.scala:37
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Got job 3 (count at DataValidators.scala:37) with 2 output partitions (allowLocal=false)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Final stage: Stage 3 (count at DataValidators.scala:37)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting Stage 3 (FilteredRDD[3] at filter at DataValidators.scala:37), which has no missing parents
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 3 (FilteredRDD[3] at filter at DataValidators.scala:37)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 3.0:0 as TID 0 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 3.0:0 as 1789 bytes in 4 ms
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 3.0:1 as TID 1 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 3.0:1 as 1789 bytes in 0 ms
14/05/14 14:54:57 INFO executor.Executor: Running task ID 0
14/05/14 14:54:57 INFO executor.Executor: Running task ID 1
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 0 is 563
14/05/14 14:54:57 INFO executor.Executor: Sending result for 0 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 0
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 1 is 563
14/05/14 14:54:57 INFO executor.Executor: Sending result for 1 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 1
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 0 in 126 ms on localhost (progress: 1/2)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 1 in 118 ms on localhost (progress: 2/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(3, 0)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(3, 1)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Stage 3 (count at DataValidators.scala:37) finished in 0.139 s
14/05/14 14:54:57 INFO spark.SparkContext: Job finished: count at DataValidators.scala:37, took 0.228246 s
14/05/14 14:54:57 INFO spark.SparkContext: Starting job: count at GradientDescent.scala:137
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Got job 4 (count at GradientDescent.scala:137) with 2 output partitions (allowLocal=false)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Final stage: Stage 4 (count at GradientDescent.scala:137)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting Stage 4 (MappedRDD[4] at map at GeneralizedLinearAlgorithm.scala:139), which has no missing parents
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 4 (MappedRDD[4] at map at GeneralizedLinearAlgorithm.scala:139)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 2 tasks
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 4.0:0 as TID 2 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 4.0:0 as 1742 bytes in 0 ms
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 4.0:1 as TID 3 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 4.0:1 as 1742 bytes in 1 ms
14/05/14 14:54:57 INFO executor.Executor: Running task ID 2
14/05/14 14:54:57 INFO executor.Executor: Running task ID 3
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 3 is 563
14/05/14 14:54:57 INFO executor.Executor: Sending result for 3 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 3
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 2 is 563
14/05/14 14:54:57 INFO executor.Executor: Sending result for 2 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 2
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(4, 1)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 3 in 55 ms on localhost (progress: 1/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(4, 0)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 2 in 58 ms on localhost (progress: 2/2)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Stage 4 (count at GradientDescent.scala:137) finished in 0.058 s
14/05/14 14:54:57 INFO spark.SparkContext: Job finished: count at GradientDescent.scala:137, took 0.065539 s
14/05/14 14:54:57 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Got job 5 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Final stage: Stage 5 (reduce at GradientDescent.scala:150)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting Stage 5 (MappedRDD[6] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 5 (MappedRDD[6] at map at GradientDescent.scala:145)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 5.0:0 as TID 4 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 5.0:0 as 2334 bytes in 0 ms
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 5.0:1 as TID 5 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 5.0:1 as 2334 bytes in 0 ms
14/05/14 14:54:57 INFO executor.Executor: Running task ID 4
14/05/14 14:54:57 INFO executor.Executor: Running task ID 5
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 4 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 4 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 5 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 5 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 5
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 4 in 51 ms on localhost (progress: 1/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(5, 0)
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 4
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 5 in 51 ms on localhost (progress: 2/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(5, 1)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Stage 5 (reduce at GradientDescent.scala:150) finished in 0.054 s
14/05/14 14:54:57 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.080316 s
14/05/14 14:54:57 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Got job 6 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Final stage: Stage 6 (reduce at GradientDescent.scala:150)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting Stage 6 (MappedRDD[8] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 6 (MappedRDD[8] at map at GradientDescent.scala:145)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 2 tasks
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 6.0:0 as TID 6 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 6.0:0 as 2476 bytes in 1 ms
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 6.0:1 as TID 7 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 6.0:1 as 2476 bytes in 1 ms
14/05/14 14:54:57 INFO executor.Executor: Running task ID 7
14/05/14 14:54:57 INFO executor.Executor: Running task ID 6
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 7 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 7 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 7
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 7 in 44 ms on localhost (progress: 1/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(6, 1)
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 6 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 6 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 6
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(6, 0)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 6 in 51 ms on localhost (progress: 2/2)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Stage 6 (reduce at GradientDescent.scala:150) finished in 0.052 s
14/05/14 14:54:57 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.061469 s
14/05/14 14:54:57 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Got job 7 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Final stage: Stage 7 (reduce at GradientDescent.scala:150)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting Stage 7 (MappedRDD[10] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 7 (MappedRDD[10] at map at GradientDescent.scala:145)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 2 tasks
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 7.0:0 as TID 8 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 7.0:0 as 2479 bytes in 0 ms
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 7.0:1 as TID 9 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 7.0:1 as 2479 bytes in 0 ms
14/05/14 14:54:57 INFO executor.Executor: Running task ID 8
14/05/14 14:54:57 INFO executor.Executor: Running task ID 9
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 9 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 9 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 9
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 8 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 8 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 8
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(7, 1)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 9 in 29 ms on localhost (progress: 1/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(7, 0)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 8 in 33 ms on localhost (progress: 2/2)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Stage 7 (reduce at GradientDescent.scala:150) finished in 0.034 s
14/05/14 14:54:57 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.043089 s
14/05/14 14:54:57 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Got job 8 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Final stage: Stage 8 (reduce at GradientDescent.scala:150)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting Stage 8 (MappedRDD[12] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 8 (MappedRDD[12] at map at GradientDescent.scala:145)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Adding task set 8.0 with 2 tasks
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 8.0:0 as TID 10 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 8.0:0 as 2476 bytes in 1 ms
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 8.0:1 as TID 11 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 8.0:1 as 2476 bytes in 1 ms
14/05/14 14:54:57 INFO executor.Executor: Running task ID 11
14/05/14 14:54:57 INFO executor.Executor: Running task ID 10
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 11 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 11 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 11
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 10 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 10 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 10
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(8, 1)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 11 in 27 ms on localhost (progress: 1/2)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 10 in 30 ms on localhost (progress: 2/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(8, 0)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Stage 8 (reduce at GradientDescent.scala:150) finished in 0.033 s
14/05/14 14:54:57 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.043034 s
14/05/14 14:54:57 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Got job 9 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Final stage: Stage 9 (reduce at GradientDescent.scala:150)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting Stage 9 (MappedRDD[14] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 9 (MappedRDD[14] at map at GradientDescent.scala:145)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Adding task set 9.0 with 2 tasks
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 9.0:0 as TID 12 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 9.0:0 as 2468 bytes in 0 ms
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 9.0:1 as TID 13 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 9.0:1 as 2468 bytes in 0 ms
14/05/14 14:54:57 INFO executor.Executor: Running task ID 12
14/05/14 14:54:57 INFO executor.Executor: Running task ID 13
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 13 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 13 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 13
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 13 in 27 ms on localhost (progress: 1/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(9, 1)
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 12 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 12 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 12
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 12 in 33 ms on localhost (progress: 2/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(9, 0)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Stage 9 (reduce at GradientDescent.scala:150) finished in 0.035 s
14/05/14 14:54:57 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.041246 s
14/05/14 14:54:57 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Got job 10 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Final stage: Stage 10 (reduce at GradientDescent.scala:150)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting Stage 10 (MappedRDD[16] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 10 (MappedRDD[16] at map at GradientDescent.scala:145)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Adding task set 10.0 with 2 tasks
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 10.0:0 as TID 14 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 10.0:0 as 2478 bytes in 0 ms
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 10.0:1 as TID 15 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 10.0:1 as 2478 bytes in 0 ms
14/05/14 14:54:57 INFO executor.Executor: Running task ID 15
14/05/14 14:54:57 INFO executor.Executor: Running task ID 14
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 15 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 15 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 15
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 15 in 26 ms on localhost (progress: 1/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(10, 1)
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 14 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 14 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 14
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 14 in 40 ms on localhost (progress: 2/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(10, 0)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Stage 10 (reduce at GradientDescent.scala:150) finished in 0.041 s
14/05/14 14:54:57 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.049353 s
14/05/14 14:54:57 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Got job 11 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Final stage: Stage 11 (reduce at GradientDescent.scala:150)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting Stage 11 (MappedRDD[18] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 11 (MappedRDD[18] at map at GradientDescent.scala:145)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Adding task set 11.0 with 2 tasks
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 11.0:0 as TID 16 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 11.0:0 as 2480 bytes in 0 ms
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 11.0:1 as TID 17 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 11.0:1 as 2480 bytes in 0 ms
14/05/14 14:54:57 INFO executor.Executor: Running task ID 17
14/05/14 14:54:57 INFO executor.Executor: Running task ID 16
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 17 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 17 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 17
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 17 in 29 ms on localhost (progress: 1/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(11, 1)
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 16 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 16 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 16
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(11, 0)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 16 in 32 ms on localhost (progress: 2/2)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Stage 11 (reduce at GradientDescent.scala:150) finished in 0.034 s
14/05/14 14:54:57 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.041377 s
14/05/14 14:54:57 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Got job 12 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Final stage: Stage 12 (reduce at GradientDescent.scala:150)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting Stage 12 (MappedRDD[20] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 12 (MappedRDD[20] at map at GradientDescent.scala:145)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Adding task set 12.0 with 2 tasks
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 12.0:0 as TID 18 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 12.0:0 as 2479 bytes in 1 ms
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 12.0:1 as TID 19 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 12.0:1 as 2479 bytes in 0 ms
14/05/14 14:54:57 INFO executor.Executor: Running task ID 19
14/05/14 14:54:57 INFO executor.Executor: Running task ID 18
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 19 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 19 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 19
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 19 in 22 ms on localhost (progress: 1/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(12, 1)
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 18 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 18 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 18
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 18 in 26 ms on localhost (progress: 2/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(12, 0)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Stage 12 (reduce at GradientDescent.scala:150) finished in 0.027 s
14/05/14 14:54:57 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.033633 s
14/05/14 14:54:57 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Got job 13 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Final stage: Stage 13 (reduce at GradientDescent.scala:150)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting Stage 13 (MappedRDD[22] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 13 (MappedRDD[22] at map at GradientDescent.scala:145)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Adding task set 13.0 with 2 tasks
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 13.0:0 as TID 20 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 13.0:0 as 2481 bytes in 0 ms
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 13.0:1 as TID 21 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 13.0:1 as 2481 bytes in 0 ms
14/05/14 14:54:57 INFO executor.Executor: Running task ID 21
14/05/14 14:54:57 INFO executor.Executor: Running task ID 20
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 21 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 21 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 21
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 21 in 24 ms on localhost (progress: 1/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(13, 1)
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 20 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 20 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 20
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 20 in 30 ms on localhost (progress: 2/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(13, 0)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Stage 13 (reduce at GradientDescent.scala:150) finished in 0.031 s
14/05/14 14:54:57 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.03787 s
14/05/14 14:54:57 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Got job 14 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Final stage: Stage 14 (reduce at GradientDescent.scala:150)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting Stage 14 (MappedRDD[24] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 14 (MappedRDD[24] at map at GradientDescent.scala:145)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Adding task set 14.0 with 2 tasks
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 14.0:0 as TID 22 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 14.0:0 as 2479 bytes in 1 ms
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 14.0:1 as TID 23 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 14.0:1 as 2479 bytes in 0 ms
14/05/14 14:54:57 INFO executor.Executor: Running task ID 22
14/05/14 14:54:57 INFO executor.Executor: Running task ID 23
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 23 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 23 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 23
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 22 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 22 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 22
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(14, 1)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 23 in 24 ms on localhost (progress: 1/2)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 22 in 27 ms on localhost (progress: 2/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(14, 0)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Stage 14 (reduce at GradientDescent.scala:150) finished in 0.028 s
14/05/14 14:54:57 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.035158 s
14/05/14 14:54:57 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Got job 15 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Final stage: Stage 15 (reduce at GradientDescent.scala:150)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting Stage 15 (MappedRDD[26] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 15 (MappedRDD[26] at map at GradientDescent.scala:145)
14/05/14 14:54:57 INFO scheduler.TaskSchedulerImpl: Adding task set 15.0 with 2 tasks
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 15.0:0 as TID 24 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 15.0:0 as 2480 bytes in 0 ms
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Starting task 15.0:1 as TID 25 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Serialized task 15.0:1 as 2480 bytes in 0 ms
14/05/14 14:54:57 INFO executor.Executor: Running task ID 24
14/05/14 14:54:57 INFO executor.Executor: Running task ID 25
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:57 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:57 INFO executor.Executor: Serialized size of result for 25 is 945
14/05/14 14:54:57 INFO executor.Executor: Sending result for 25 directly to driver
14/05/14 14:54:57 INFO executor.Executor: Finished task ID 25
14/05/14 14:54:57 INFO scheduler.TaskSetManager: Finished TID 25 in 19 ms on localhost (progress: 1/2)
14/05/14 14:54:57 INFO scheduler.DAGScheduler: Completed ResultTask(15, 1)
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 24 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 24 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 24
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 24 in 26 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(15, 0)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 15 (reduce at GradientDescent.scala:150) finished in 0.027 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.033397 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 16 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 16 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 16 (MappedRDD[28] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 16 (MappedRDD[28] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 16.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 16.0:0 as TID 26 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 16.0:0 as 2482 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 16.0:1 as TID 27 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 16.0:1 as 2482 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 27
14/05/14 14:54:58 INFO executor.Executor: Running task ID 26
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 26 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 26 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 26
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 27 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 27 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 27
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(16, 0)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 26 in 26 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 27 in 26 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(16, 1)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 16 (reduce at GradientDescent.scala:150) finished in 0.028 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.033781 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 17 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 17 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 17 (MappedRDD[30] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 17 (MappedRDD[30] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 17.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 17.0:0 as TID 28 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 17.0:0 as 2480 bytes in 1 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 17.0:1 as TID 29 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 17.0:1 as 2480 bytes in 1 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 28
14/05/14 14:54:58 INFO executor.Executor: Running task ID 29
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 28 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 28 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 28
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 28 in 22 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(17, 0)
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 29 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 29 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 29
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 29 in 27 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(17, 1)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 17 (reduce at GradientDescent.scala:150) finished in 0.029 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.038229 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 18 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 18 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 18 (MappedRDD[32] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 18 (MappedRDD[32] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 18.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 18.0:0 as TID 30 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 18.0:0 as 2481 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 18.0:1 as TID 31 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 18.0:1 as 2481 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 31
14/05/14 14:54:58 INFO executor.Executor: Running task ID 30
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 30 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 30 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 31 is 945
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 30
14/05/14 14:54:58 INFO executor.Executor: Sending result for 31 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 31
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 30 in 22 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(18, 0)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 31 in 22 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(18, 1)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 18 (reduce at GradientDescent.scala:150) finished in 0.024 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.032879 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 19 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 19 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 19 (MappedRDD[34] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 19 (MappedRDD[34] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 19.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 19.0:0 as TID 32 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 19.0:0 as 2482 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 19.0:1 as TID 33 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 19.0:1 as 2482 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 33
14/05/14 14:54:58 INFO executor.Executor: Running task ID 32
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 32 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 32 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 32
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 33 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 33 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 33
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(19, 0)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 32 in 20 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 33 in 20 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(19, 1)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 19 (reduce at GradientDescent.scala:150) finished in 0.022 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.02877 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 20 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 20 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 20 (MappedRDD[36] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 20 (MappedRDD[36] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 20.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 20.0:0 as TID 34 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 20.0:0 as 2480 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 20.0:1 as TID 35 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 20.0:1 as 2480 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 34
14/05/14 14:54:58 INFO executor.Executor: Running task ID 35
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 34 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 34 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 34
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 34 in 21 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(20, 0)
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 35 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 35 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 35
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 35 in 28 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(20, 1)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 20 (reduce at GradientDescent.scala:150) finished in 0.031 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.039071 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 21 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 21 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 21 (MappedRDD[38] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 21 (MappedRDD[38] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 21.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 21.0:0 as TID 36 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 21.0:0 as 2481 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 21.0:1 as TID 37 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 21.0:1 as 2481 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 36
14/05/14 14:54:58 INFO executor.Executor: Running task ID 37
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 37 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 37 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 37
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(21, 1)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 37 in 17 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 36 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 36 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 36
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 36 in 20 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(21, 0)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 21 (reduce at GradientDescent.scala:150) finished in 0.022 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.031868 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 22 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 22 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 22 (MappedRDD[40] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 22 (MappedRDD[40] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 22.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 22.0:0 as TID 38 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 22.0:0 as 2480 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 22.0:1 as TID 39 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 22.0:1 as 2480 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 39
14/05/14 14:54:58 INFO executor.Executor: Running task ID 38
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 39 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 39 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 39
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 39 in 17 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(22, 1)
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 38 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 38 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 38
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 38 in 20 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(22, 0)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 22 (reduce at GradientDescent.scala:150) finished in 0.022 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.029293 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 23 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 23 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 23 (MappedRDD[42] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 23 (MappedRDD[42] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 23.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 23.0:0 as TID 40 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 23.0:0 as 2480 bytes in 1 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 23.0:1 as TID 41 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 23.0:1 as 2480 bytes in 1 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 40
14/05/14 14:54:58 INFO executor.Executor: Running task ID 41
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 41 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 41 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 41
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 40 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 40 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 40
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 41 in 19 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(23, 1)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 40 in 22 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(23, 0)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 23 (reduce at GradientDescent.scala:150) finished in 0.023 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.029613 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 24 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 24 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 24 (MappedRDD[44] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 24 (MappedRDD[44] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 24.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 24.0:0 as TID 42 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 24.0:0 as 2479 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 24.0:1 as TID 43 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 24.0:1 as 2479 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 42
14/05/14 14:54:58 INFO executor.Executor: Running task ID 43
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 42 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 42 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 43 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 43 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 42
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 43
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 42 in 19 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(24, 0)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 43 in 20 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(24, 1)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 24 (reduce at GradientDescent.scala:150) finished in 0.022 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.029476 s
14/05/14 14:54:58 INFO optimization.GradientDescent: GradientDescent finished. Last 10 stochastic losses 0.9742920809409373, 0.974292080896445, 0.974292080889609, 0.9742920808884172, 0.9742920808881859, 0.9742920808881375, 0.9742920808881257, 0.9742920808881232, 0.9742920808881225, 0.9742920808881221
14/05/14 14:54:58 INFO classification.SVMWithSGD: Final weights -0.044374814180854705,0.011742784807525528,-0.025607375986579188,0.06916056951785765,-0.03684047248061573,0.01245145612729382,0.05591032824042298,-0.07582051391118914,0.02076136996343498,0.020761369963434986,0.05323971374999502,-0.03829197905337417,0.03952846845592045,-0.016162706473643828,-2.296184296094748E-9,0.02790727536706444
14/05/14 14:54:58 INFO classification.SVMWithSGD: Final intercept 0.0031055898817100042
-------------------------------------------------
>> model: org.apache.spark.mllib.classification.SVMModel@2cd9799d
evaluateModel - First
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: count at BinaryClassification_tut1.scala:45
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 25 (count at BinaryClassification_tut1.scala:45) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 25 (count at BinaryClassification_tut1.scala:45)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 25 (FilteredRDD[46] at filter at BinaryClassification_tut1.scala:45), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 25 (FilteredRDD[46] at filter at BinaryClassification_tut1.scala:45)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 25.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 25.0:0 as TID 44 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 25.0:0 as 2139 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 25.0:1 as TID 45 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 25.0:1 as 2139 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 44
14/05/14 14:54:58 INFO executor.Executor: Running task ID 45
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 44 is 563
14/05/14 14:54:58 INFO executor.Executor: Sending result for 44 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 45 is 563
14/05/14 14:54:58 INFO executor.Executor: Sending result for 45 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 45
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 44 in 24 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(25, 0)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(25, 1)
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 44
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 45 in 24 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 25 (count at BinaryClassification_tut1.scala:45) finished in 0.026 s
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: count at BinaryClassification_tut1.scala:45, took 0.034555 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: count at BinaryClassification_tut1.scala:45
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 26 (count at BinaryClassification_tut1.scala:45) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 26 (count at BinaryClassification_tut1.scala:45)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 26 (MappedRDD[2] at map at BinaryClassification_tut1.scala:19), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 26 (MappedRDD[2] at map at BinaryClassification_tut1.scala:19)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 26.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 26.0:0 as TID 46 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 26.0:0 as 1704 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 26.0:1 as TID 47 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 26.0:1 as 1704 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 46
14/05/14 14:54:58 INFO executor.Executor: Running task ID 47
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 46 is 563
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 47 is 563
14/05/14 14:54:58 INFO executor.Executor: Sending result for 47 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Sending result for 46 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 47
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 46
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 47 in 14 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(26, 1)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(26, 0)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 46 in 15 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 26 (count at BinaryClassification_tut1.scala:45) finished in 0.016 s
training Error for First model : 0.38819875776397517
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: count at BinaryClassification_tut1.scala:45, took 0.021437 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: first at GeneralizedLinearAlgorithm.scala:121
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 27 (first at GeneralizedLinearAlgorithm.scala:121) with 1 output partitions (allowLocal=true)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 27 (first at GeneralizedLinearAlgorithm.scala:121)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Computing the requested partition locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: first at GeneralizedLinearAlgorithm.scala:121, took 0.003556 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: count at DataValidators.scala:37
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 28 (count at DataValidators.scala:37) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 28 (count at DataValidators.scala:37)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 28 (FilteredRDD[47] at filter at DataValidators.scala:37), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 28 (FilteredRDD[47] at filter at DataValidators.scala:37)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 28.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 28.0:0 as TID 48 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 28.0:0 as 1788 bytes in 1 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 28.0:1 as TID 49 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 28.0:1 as 1788 bytes in 1 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 48
14/05/14 14:54:58 INFO executor.Executor: Running task ID 49
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 48 is 563
14/05/14 14:54:58 INFO executor.Executor: Sending result for 48 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 48
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 48 in 15 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(28, 0)
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 49 is 563
14/05/14 14:54:58 INFO executor.Executor: Sending result for 49 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 49
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 49 in 16 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(28, 1)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 28 (count at DataValidators.scala:37) finished in 0.018 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: count at DataValidators.scala:37, took 0.02546 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: count at GradientDescent.scala:137
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 29 (count at GradientDescent.scala:137) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 29 (count at GradientDescent.scala:137)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 29 (MappedRDD[48] at map at GeneralizedLinearAlgorithm.scala:139), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 29 (MappedRDD[48] at map at GeneralizedLinearAlgorithm.scala:139)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 29.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 29.0:0 as TID 50 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 29.0:0 as 1741 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 29.0:1 as TID 51 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 29.0:1 as 1741 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 51
14/05/14 14:54:58 INFO executor.Executor: Running task ID 50
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 50 is 563
14/05/14 14:54:58 INFO executor.Executor: Sending result for 50 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 51 is 563
14/05/14 14:54:58 INFO executor.Executor: Sending result for 51 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 51
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 50
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 50 in 15 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(29, 0)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 51 in 15 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(29, 1)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 29 (count at GradientDescent.scala:137) finished in 0.017 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: count at GradientDescent.scala:137, took 0.021837 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 30 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 30 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 30 (MappedRDD[50] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 30 (MappedRDD[50] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 30.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 30.0:0 as TID 52 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 30.0:0 as 2333 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 30.0:1 as TID 53 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 30.0:1 as 2333 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 53
14/05/14 14:54:58 INFO executor.Executor: Running task ID 52
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 53 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 53 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 53
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 53 in 17 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(30, 1)
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 52 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 52 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 52
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 52 in 21 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(30, 0)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 30 (reduce at GradientDescent.scala:150) finished in 0.023 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.030384 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 31 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 31 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 31 (MappedRDD[52] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 31 (MappedRDD[52] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 31.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 31.0:0 as TID 54 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 31.0:0 as 2384 bytes in 1 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 31.0:1 as TID 55 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 31.0:1 as 2384 bytes in 1 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 54
14/05/14 14:54:58 INFO executor.Executor: Running task ID 55
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 55 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 55 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 55
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 55 in 24 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 54 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 54 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 54
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(31, 1)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 54 in 28 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(31, 0)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 31 (reduce at GradientDescent.scala:150) finished in 0.029 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.03697 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 32 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 32 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 32 (MappedRDD[54] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 32 (MappedRDD[54] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 32.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 32.0:0 as TID 56 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 32.0:0 as 2386 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 32.0:1 as TID 57 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 32.0:1 as 2386 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 56
14/05/14 14:54:58 INFO executor.Executor: Running task ID 57
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 57 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 57 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 57
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(32, 1)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 57 in 15 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 56 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 56 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 56
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(32, 0)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 56 in 18 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 32 (reduce at GradientDescent.scala:150) finished in 0.020 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.026297 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 33 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 33 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 33 (MappedRDD[56] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 33 (MappedRDD[56] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 33.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 33.0:0 as TID 58 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 33.0:0 as 2385 bytes in 1 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 33.0:1 as TID 59 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 33.0:1 as 2385 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 59
14/05/14 14:54:58 INFO executor.Executor: Running task ID 58
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 59 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 59 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 59
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 58 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 58 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 58
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 59 in 19 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 58 in 20 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(33, 1)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(33, 0)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 33 (reduce at GradientDescent.scala:150) finished in 0.022 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.028554 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 34 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 34 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 34 (MappedRDD[58] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 34 (MappedRDD[58] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 34.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 34.0:0 as TID 60 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 34.0:0 as 2385 bytes in 1 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 34.0:1 as TID 61 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 34.0:1 as 2385 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 60
14/05/14 14:54:58 INFO executor.Executor: Running task ID 61
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 60 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 60 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 60
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 60 in 17 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(34, 0)
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 61 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 61 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 61
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 61 in 20 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(34, 1)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 34 (reduce at GradientDescent.scala:150) finished in 0.022 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.031106 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 35 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 35 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 35 (MappedRDD[60] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 35 (MappedRDD[60] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 35.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 35.0:0 as TID 62 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 35.0:0 as 2385 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 35.0:1 as TID 63 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 35.0:1 as 2385 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 62
14/05/14 14:54:58 INFO executor.Executor: Running task ID 63
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 62 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 62 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 62
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 63 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 63 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 63
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 62 in 17 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(35, 0)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 63 in 17 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(35, 1)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 35 (reduce at GradientDescent.scala:150) finished in 0.019 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.026063 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 36 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 36 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 36 (MappedRDD[62] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 36 (MappedRDD[62] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 36.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 36.0:0 as TID 64 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 36.0:0 as 2385 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 36.0:1 as TID 65 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 36.0:1 as 2385 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 64
14/05/14 14:54:58 INFO executor.Executor: Running task ID 65
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 64 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 64 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 64
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(36, 0)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 64 in 19 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 65 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 65 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 65
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 65 in 21 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(36, 1)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 36 (reduce at GradientDescent.scala:150) finished in 0.022 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.031107 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 37 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 37 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 37 (MappedRDD[64] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 37 (MappedRDD[64] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 37.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 37.0:0 as TID 66 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 37.0:0 as 2387 bytes in 1 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 37.0:1 as TID 67 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 37.0:1 as 2387 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 67
14/05/14 14:54:58 INFO executor.Executor: Running task ID 66
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 67 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 67 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 67
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 66 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 66 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 66
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 67 in 13 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(37, 1)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(37, 0)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 66 in 15 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 37 (reduce at GradientDescent.scala:150) finished in 0.015 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.022218 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 38 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 38 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 38 (MappedRDD[66] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 38 (MappedRDD[66] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 38.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 38.0:0 as TID 68 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 38.0:0 as 2384 bytes in 1 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 38.0:1 as TID 69 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 38.0:1 as 2384 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 68
14/05/14 14:54:58 INFO executor.Executor: Running task ID 69
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 69 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 69 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 69
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 68 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 68 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 68
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(38, 1)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 69 in 12 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 68 in 15 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(38, 0)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 38 (reduce at GradientDescent.scala:150) finished in 0.016 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.022583 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 39 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 39 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 39 (MappedRDD[68] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 39 (MappedRDD[68] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 39.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 39.0:0 as TID 70 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 39.0:0 as 2386 bytes in 1 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 39.0:1 as TID 71 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 39.0:1 as 2386 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 70
14/05/14 14:54:58 INFO executor.Executor: Running task ID 71
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 70 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 70 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 70
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(39, 0)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 70 in 12 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 71 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 71 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 71
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(39, 1)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 39 (reduce at GradientDescent.scala:150) finished in 0.015 s
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 71 in 14 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.022886 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 40 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 40 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 40 (MappedRDD[70] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 40 (MappedRDD[70] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 40.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 40.0:0 as TID 72 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 40.0:0 as 2385 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 40.0:1 as TID 73 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 40.0:1 as 2385 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 72
14/05/14 14:54:58 INFO executor.Executor: Running task ID 73
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 72 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 72 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 72
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 73 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 73 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 73
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(40, 0)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 72 in 11 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(40, 1)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 40 (reduce at GradientDescent.scala:150) finished in 0.013 s
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 73 in 11 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.019595 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 41 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 41 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 41 (MappedRDD[72] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 41 (MappedRDD[72] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 41.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 41.0:0 as TID 74 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 41.0:0 as 2387 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 41.0:1 as TID 75 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 41.0:1 as 2387 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 74
14/05/14 14:54:58 INFO executor.Executor: Running task ID 75
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 74 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 74 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 75 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 75 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 75
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 74
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(41, 0)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 74 in 14 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(41, 1)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 75 in 14 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 41 (reduce at GradientDescent.scala:150) finished in 0.016 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.022847 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 42 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 42 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 42 (MappedRDD[74] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 42 (MappedRDD[74] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 42.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 42.0:0 as TID 76 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 42.0:0 as 2386 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 42.0:1 as TID 77 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 42.0:1 as 2386 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 76
14/05/14 14:54:58 INFO executor.Executor: Running task ID 77
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 77 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 77 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 76 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 76 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 77
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 76
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(42, 1)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 77 in 10 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(42, 0)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 76 in 11 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 42 (reduce at GradientDescent.scala:150) finished in 0.013 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.019814 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 43 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 43 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 43 (MappedRDD[76] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 43 (MappedRDD[76] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 43.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 43.0:0 as TID 78 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 43.0:0 as 2388 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 43.0:1 as TID 79 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 43.0:1 as 2388 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 78
14/05/14 14:54:58 INFO executor.Executor: Running task ID 79
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 78 is 945
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 79 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 78 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Sending result for 79 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 79
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 78
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 78 in 11 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(43, 0)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 79 in 11 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(43, 1)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 43 (reduce at GradientDescent.scala:150) finished in 0.013 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.019019 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 44 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 44 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 44 (MappedRDD[78] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 44 (MappedRDD[78] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 44.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 44.0:0 as TID 80 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 44.0:0 as 2386 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 44.0:1 as TID 81 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 44.0:1 as 2386 bytes in 0 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 80
14/05/14 14:54:58 INFO executor.Executor: Running task ID 81
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 80 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 80 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 81 is 945
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 80
14/05/14 14:54:58 INFO executor.Executor: Sending result for 81 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 81
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 80 in 11 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(44, 0)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 81 in 11 ms on localhost (progress: 2/2)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(44, 1)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Stage 44 (reduce at GradientDescent.scala:150) finished in 0.013 s
14/05/14 14:54:58 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.021399 s
14/05/14 14:54:58 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Got job 45 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Final stage: Stage 45 (reduce at GradientDescent.scala:150)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting Stage 45 (MappedRDD[80] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 45 (MappedRDD[80] at map at GradientDescent.scala:145)
14/05/14 14:54:58 INFO scheduler.TaskSchedulerImpl: Adding task set 45.0 with 2 tasks
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 45.0:0 as TID 82 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 45.0:0 as 2386 bytes in 0 ms
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Starting task 45.0:1 as TID 83 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Serialized task 45.0:1 as 2386 bytes in 1 ms
14/05/14 14:54:58 INFO executor.Executor: Running task ID 82
14/05/14 14:54:58 INFO executor.Executor: Running task ID 83
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:58 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 82 is 945
14/05/14 14:54:58 INFO executor.Executor: Serialized size of result for 83 is 945
14/05/14 14:54:58 INFO executor.Executor: Sending result for 83 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Sending result for 82 directly to driver
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 83
14/05/14 14:54:58 INFO executor.Executor: Finished task ID 82
14/05/14 14:54:58 INFO scheduler.TaskSetManager: Finished TID 83 in 10 ms on localhost (progress: 1/2)
14/05/14 14:54:58 INFO scheduler.DAGScheduler: Completed ResultTask(45, 1)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 82 in 12 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(45, 0)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 45 (reduce at GradientDescent.scala:150) finished in 0.013 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.020724 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 46 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 46 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 46 (MappedRDD[82] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 46 (MappedRDD[82] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 46.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 46.0:0 as TID 84 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 46.0:0 as 2384 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 46.0:1 as TID 85 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 46.0:1 as 2384 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 84
14/05/14 14:54:59 INFO executor.Executor: Running task ID 85
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 84 is 945
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 85 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 84 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Sending result for 85 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 84
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 85
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 84 in 12 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(46, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 85 in 13 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(46, 1)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 46 (reduce at GradientDescent.scala:150) finished in 0.015 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.022189 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 47 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 47 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 47 (MappedRDD[84] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 47 (MappedRDD[84] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 47.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 47.0:0 as TID 86 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 47.0:0 as 2386 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 47.0:1 as TID 87 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 47.0:1 as 2386 bytes in 1 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 86
14/05/14 14:54:59 INFO executor.Executor: Running task ID 87
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 86 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 86 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 86
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 87 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 87 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 87
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 86 in 11 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(47, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 87 in 11 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(47, 1)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 47 (reduce at GradientDescent.scala:150) finished in 0.014 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.020045 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 48 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 48 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 48 (MappedRDD[86] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 48 (MappedRDD[86] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 48.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 48.0:0 as TID 88 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 48.0:0 as 2386 bytes in 1 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 48.0:1 as TID 89 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 48.0:1 as 2386 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 88
14/05/14 14:54:59 INFO executor.Executor: Running task ID 89
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 88 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 88 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 89 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 89 directly to driver
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 88 in 10 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(48, 0)
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 88
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 89
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 89 in 10 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(48, 1)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 48 (reduce at GradientDescent.scala:150) finished in 0.012 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.019556 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 49 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 49 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 49 (MappedRDD[88] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 49 (MappedRDD[88] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 49.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 49.0:0 as TID 90 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 49.0:0 as 2386 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 49.0:1 as TID 91 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 49.0:1 as 2386 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 90
14/05/14 14:54:59 INFO executor.Executor: Running task ID 91
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 90 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 90 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 91 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 91 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 91
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 90
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 90 in 12 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(49, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 91 in 12 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(49, 1)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 49 (reduce at GradientDescent.scala:150) finished in 0.015 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.020791 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 50 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 50 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 50 (MappedRDD[90] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 50 (MappedRDD[90] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 50.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 50.0:0 as TID 92 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 50.0:0 as 2388 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 50.0:1 as TID 93 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 50.0:1 as 2388 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 92
14/05/14 14:54:59 INFO executor.Executor: Running task ID 93
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 93 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 93 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 93
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 92 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 92 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 92
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(50, 1)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 93 in 10 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(50, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 92 in 11 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 50 (reduce at GradientDescent.scala:150) finished in 0.013 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.018979 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 51 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 51 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 51 (MappedRDD[92] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 51 (MappedRDD[92] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 51.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 51.0:0 as TID 94 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 51.0:0 as 2386 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 51.0:1 as TID 95 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 51.0:1 as 2386 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 94
14/05/14 14:54:59 INFO executor.Executor: Running task ID 95
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 94 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 94 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 94
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(51, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 94 in 49 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 95 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 95 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 95
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(51, 1)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 95 in 49 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 51 (reduce at GradientDescent.scala:150) finished in 0.051 s
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.057321 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 52 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 52 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 52 (MappedRDD[94] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 52 (MappedRDD[94] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 52.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 52.0:0 as TID 96 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 52.0:0 as 2386 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 52.0:1 as TID 97 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 52.0:1 as 2386 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 96
14/05/14 14:54:59 INFO executor.Executor: Running task ID 97
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 96 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 96 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 96
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 97 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 97 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 97
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(52, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 96 in 9 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 97 in 9 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(52, 1)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 52 (reduce at GradientDescent.scala:150) finished in 0.011 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.017742 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 53 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 53 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 53 (MappedRDD[96] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 53 (MappedRDD[96] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 53.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 53.0:0 as TID 98 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 53.0:0 as 2385 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 53.0:1 as TID 99 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 53.0:1 as 2385 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 98
14/05/14 14:54:59 INFO executor.Executor: Running task ID 99
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 98 is 945
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 99 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 98 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Sending result for 99 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 98
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 99
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(53, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 98 in 10 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 99 in 10 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(53, 1)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 53 (reduce at GradientDescent.scala:150) finished in 0.013 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.018786 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 54 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 54 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 54 (MappedRDD[98] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 54 (MappedRDD[98] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 54.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 54.0:0 as TID 100 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 54.0:0 as 2384 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 54.0:1 as TID 101 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 54.0:1 as 2384 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 100
14/05/14 14:54:59 INFO executor.Executor: Running task ID 101
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 100 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 100 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 101 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 101 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 101
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 100
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 100 in 9 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(54, 0)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(54, 1)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 101 in 9 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 54 (reduce at GradientDescent.scala:150) finished in 0.011 s
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.016964 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 55 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 55 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 55 (MappedRDD[100] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 55 (MappedRDD[100] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 55.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 55.0:0 as TID 102 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 55.0:0 as 2385 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 55.0:1 as TID 103 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 55.0:1 as 2385 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 102
14/05/14 14:54:59 INFO executor.Executor: Running task ID 103
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 103 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 103 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 102 is 945
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 103
14/05/14 14:54:59 INFO executor.Executor: Sending result for 102 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 102
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 103 in 9 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(55, 1)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(55, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 102 in 10 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 55 (reduce at GradientDescent.scala:150) finished in 0.011 s
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.017277 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 56 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 56 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 56 (MappedRDD[102] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 56 (MappedRDD[102] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 56.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 56.0:0 as TID 104 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 56.0:0 as 2385 bytes in 1 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 56.0:1 as TID 105 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 56.0:1 as 2385 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 104
14/05/14 14:54:59 INFO executor.Executor: Running task ID 105
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 104 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 104 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 105 is 945
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 104
14/05/14 14:54:59 INFO executor.Executor: Sending result for 105 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 105
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 104 in 27 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(56, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 105 in 27 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(56, 1)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 56 (reduce at GradientDescent.scala:150) finished in 0.029 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.035732 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 57 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 57 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 57 (MappedRDD[104] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 57 (MappedRDD[104] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 57.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 57.0:0 as TID 106 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 57.0:0 as 2431 bytes in 1 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 57.0:1 as TID 107 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 57.0:1 as 2431 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 106
14/05/14 14:54:59 INFO executor.Executor: Running task ID 107
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 107 is 945
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 106 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 107 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Sending result for 106 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 107
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 106
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 107 in 8 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(57, 1)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(57, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 106 in 10 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 57 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.016321 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 58 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 58 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 58 (MappedRDD[106] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 58 (MappedRDD[106] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 58.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 58.0:0 as TID 108 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 58.0:0 as 2399 bytes in 1 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 58.0:1 as TID 109 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 58.0:1 as 2399 bytes in 1 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 108
14/05/14 14:54:59 INFO executor.Executor: Running task ID 109
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 108 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 108 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 109 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 109 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 109
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 108
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(58, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 108 in 9 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 109 in 9 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(58, 1)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 58 (reduce at GradientDescent.scala:150) finished in 0.011 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.017088 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 59 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 59 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 59 (MappedRDD[108] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 59 (MappedRDD[108] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 59.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 59.0:0 as TID 110 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 59.0:0 as 2411 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 59.0:1 as TID 111 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 59.0:1 as 2411 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 111
14/05/14 14:54:59 INFO executor.Executor: Running task ID 110
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 110 is 945
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 111 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 110 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Sending result for 111 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 111
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 110
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 110 in 9 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(59, 0)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(59, 1)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 111 in 9 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 59 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.016584 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 60 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 60 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 60 (MappedRDD[110] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 60 (MappedRDD[110] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 60.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 60.0:0 as TID 112 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 60.0:0 as 2400 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 60.0:1 as TID 113 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 60.0:1 as 2400 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 112
14/05/14 14:54:59 INFO executor.Executor: Running task ID 113
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 112 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 112 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 112
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 113 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 113 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 113
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(60, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 112 in 10 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 113 in 10 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(60, 1)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 60 (reduce at GradientDescent.scala:150) finished in 0.012 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.017822 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 61 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 61 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 61 (MappedRDD[112] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 61 (MappedRDD[112] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 61.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 61.0:0 as TID 114 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 61.0:0 as 2397 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 61.0:1 as TID 115 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 61.0:1 as 2397 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 114
14/05/14 14:54:59 INFO executor.Executor: Running task ID 115
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 114 is 945
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 115 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 114 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Sending result for 115 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 114
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 115
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 114 in 11 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(61, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 115 in 11 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(61, 1)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 61 (reduce at GradientDescent.scala:150) finished in 0.013 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.019102 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 62 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 62 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 62 (MappedRDD[114] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 62 (MappedRDD[114] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 62.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 62.0:0 as TID 116 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 62.0:0 as 2398 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 62.0:1 as TID 117 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 62.0:1 as 2398 bytes in 1 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 117
14/05/14 14:54:59 INFO executor.Executor: Running task ID 116
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 117 is 945
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 116 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 117 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Sending result for 116 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 117
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 116
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 117 in 8 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(62, 1)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(62, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 116 in 9 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 62 (reduce at GradientDescent.scala:150) finished in 0.011 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.016583 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 63 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 63 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 63 (MappedRDD[116] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 63 (MappedRDD[116] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 63.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 63.0:0 as TID 118 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 63.0:0 as 2392 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 63.0:1 as TID 119 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 63.0:1 as 2392 bytes in 1 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 118
14/05/14 14:54:59 INFO executor.Executor: Running task ID 119
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 118 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 118 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 119 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 119 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 119
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 118
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(63, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 118 in 9 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 119 in 9 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(63, 1)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 63 (reduce at GradientDescent.scala:150) finished in 0.011 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.017205 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 64 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 64 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 64 (MappedRDD[118] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 64 (MappedRDD[118] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 64.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 64.0:0 as TID 120 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 64.0:0 as 2387 bytes in 1 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 64.0:1 as TID 121 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 64.0:1 as 2387 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 120
14/05/14 14:54:59 INFO executor.Executor: Running task ID 121
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 120 is 945
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 121 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 120 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Sending result for 121 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 121
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 120
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 120 in 8 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(64, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 121 in 8 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(64, 1)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 64 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.016459 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 65 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 65 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 65 (MappedRDD[120] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 65 (MappedRDD[120] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 65.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 65.0:0 as TID 122 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 65.0:0 as 2384 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 65.0:1 as TID 123 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 65.0:1 as 2384 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 122
14/05/14 14:54:59 INFO executor.Executor: Running task ID 123
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 122 is 945
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 123 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 122 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Sending result for 123 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 122
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 123
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 122 in 25 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(65, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 123 in 25 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(65, 1)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 65 (reduce at GradientDescent.scala:150) finished in 0.027 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.032394 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 66 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 66 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 66 (MappedRDD[122] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 66 (MappedRDD[122] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 66.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 66.0:0 as TID 124 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 66.0:0 as 2391 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 66.0:1 as TID 125 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 66.0:1 as 2391 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 124
14/05/14 14:54:59 INFO executor.Executor: Running task ID 125
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 124 is 945
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 125 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 124 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Sending result for 125 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 125
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 124
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 124 in 8 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(66, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 125 in 8 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(66, 1)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 66 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.015895 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 67 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 67 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 67 (MappedRDD[124] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 67 (MappedRDD[124] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 67.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 67.0:0 as TID 126 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 67.0:0 as 2387 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 67.0:1 as TID 127 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 67.0:1 as 2387 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 126
14/05/14 14:54:59 INFO executor.Executor: Running task ID 127
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 126 is 945
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 127 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 126 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Sending result for 127 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 127
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 126
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 126 in 8 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(67, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 127 in 8 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(67, 1)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 67 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.017119 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 68 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 68 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 68 (MappedRDD[126] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 68 (MappedRDD[126] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 68.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 68.0:0 as TID 128 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 68.0:0 as 2391 bytes in 1 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 68.0:1 as TID 129 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 68.0:1 as 2391 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 128
14/05/14 14:54:59 INFO executor.Executor: Running task ID 129
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 128 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 128 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 129 is 945
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 128
14/05/14 14:54:59 INFO executor.Executor: Sending result for 129 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 129
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 128 in 9 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(68, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 129 in 9 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(68, 1)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 68 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.016867 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 69 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 69 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 69 (MappedRDD[128] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 69 (MappedRDD[128] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 69.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 69.0:0 as TID 130 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 69.0:0 as 2387 bytes in 1 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 69.0:1 as TID 131 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 69.0:1 as 2387 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 130
14/05/14 14:54:59 INFO executor.Executor: Running task ID 131
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 131 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 131 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 131
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 130 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 130 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 130
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 131 in 8 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(69, 1)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 130 in 10 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(69, 0)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 69 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.017444 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 70 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 70 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 70 (MappedRDD[130] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 70 (MappedRDD[130] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 70.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 70.0:0 as TID 132 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 70.0:0 as 2391 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 70.0:1 as TID 133 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 70.0:1 as 2391 bytes in 1 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 132
14/05/14 14:54:59 INFO executor.Executor: Running task ID 133
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 132 is 945
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 133 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 132 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Sending result for 133 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 132
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 133
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 132 in 23 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(70, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 133 in 25 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(70, 1)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 70 (reduce at GradientDescent.scala:150) finished in 0.027 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.032584 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 71 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 71 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 71 (MappedRDD[132] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 71 (MappedRDD[132] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 71.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 71.0:0 as TID 134 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 71.0:0 as 2386 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 71.0:1 as TID 135 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 71.0:1 as 2386 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 134
14/05/14 14:54:59 INFO executor.Executor: Running task ID 135
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 134 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 134 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 134
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 134 in 9 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(71, 0)
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 135 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 135 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 135
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 135 in 9 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(71, 1)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 71 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.016923 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 72 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 72 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 72 (MappedRDD[134] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 72 (MappedRDD[134] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 72.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 72.0:0 as TID 136 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 72.0:0 as 2390 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 72.0:1 as TID 137 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 72.0:1 as 2390 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 137
14/05/14 14:54:59 INFO executor.Executor: Running task ID 136
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 136 is 945
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 137 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 136 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Sending result for 137 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 136
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 137
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 136 in 8 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(72, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 137 in 8 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(72, 1)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 72 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.016728 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 73 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 73 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 73 (MappedRDD[136] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 73 (MappedRDD[136] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 73.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 73.0:0 as TID 138 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 73.0:0 as 2385 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 73.0:1 as TID 139 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 73.0:1 as 2385 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 138
14/05/14 14:54:59 INFO executor.Executor: Running task ID 139
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 139 is 945
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 138 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 139 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Sending result for 138 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 138
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 139
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 139 in 7 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(73, 1)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 138 in 9 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(73, 0)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 73 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.017397 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 74 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 74 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 74 (MappedRDD[138] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 74 (MappedRDD[138] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 74.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 74.0:0 as TID 140 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 74.0:0 as 2391 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 74.0:1 as TID 141 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 74.0:1 as 2391 bytes in 1 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 140
14/05/14 14:54:59 INFO executor.Executor: Running task ID 141
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 140 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 140 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 141 is 945
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 140
14/05/14 14:54:59 INFO executor.Executor: Sending result for 141 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 141
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 140 in 8 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(74, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 141 in 8 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(74, 1)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 74 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.015181 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 75 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 75 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 75 (MappedRDD[140] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 75 (MappedRDD[140] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 75.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 75.0:0 as TID 142 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 75.0:0 as 2384 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 75.0:1 as TID 143 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 75.0:1 as 2384 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 142
14/05/14 14:54:59 INFO executor.Executor: Running task ID 143
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 142 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 142 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 143 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 143 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 143
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 142 in 12 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(75, 0)
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 142
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 143 in 12 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(75, 1)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 75 (reduce at GradientDescent.scala:150) finished in 0.014 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.020932 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 76 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 76 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 76 (MappedRDD[142] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 76 (MappedRDD[142] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 76.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 76.0:0 as TID 144 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 76.0:0 as 2392 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 76.0:1 as TID 145 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 76.0:1 as 2392 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 144
14/05/14 14:54:59 INFO executor.Executor: Running task ID 145
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 145 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 145 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 145
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 144 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 144 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 144
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 145 in 6 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(76, 1)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 144 in 8 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(76, 0)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 76.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 76 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.015046 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 77 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 77 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 77 (MappedRDD[144] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 77 (MappedRDD[144] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 77.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 77.0:0 as TID 146 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 77.0:0 as 2385 bytes in 1 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 77.0:1 as TID 147 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 77.0:1 as 2385 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 146
14/05/14 14:54:59 INFO executor.Executor: Running task ID 147
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 147 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 147 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 146 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 146 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 146
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 147
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 147 in 7 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(77, 1)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 146 in 9 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(77, 0)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 77 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.016021 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 78 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 78 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 78 (MappedRDD[146] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 78 (MappedRDD[146] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 78.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 78.0:0 as TID 148 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 78.0:0 as 2427 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 78.0:1 as TID 149 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 78.0:1 as 2427 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 148
14/05/14 14:54:59 INFO executor.Executor: Running task ID 149
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 148 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 148 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 149 is 945
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 148
14/05/14 14:54:59 INFO executor.Executor: Sending result for 149 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 149
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 148 in 8 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(78, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 149 in 7 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(78, 1)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 78 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.014607 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 79 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 79 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 79 (MappedRDD[148] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 79 (MappedRDD[148] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 79.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 79.0:0 as TID 150 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 79.0:0 as 2404 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 79.0:1 as TID 151 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 79.0:1 as 2404 bytes in 1 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 150
14/05/14 14:54:59 INFO executor.Executor: Running task ID 151
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 151 is 945
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 150 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 151 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Sending result for 150 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 151
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 150
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 151 in 7 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(79, 1)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 150 in 9 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(79, 0)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 79 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.01578 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 80 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 80 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 80 (MappedRDD[150] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 80 (MappedRDD[150] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 80.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 80.0:0 as TID 152 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 80.0:0 as 2405 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 80.0:1 as TID 153 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 80.0:1 as 2405 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 152
14/05/14 14:54:59 INFO executor.Executor: Running task ID 153
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 152 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 152 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 153 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 153 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 152
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 153
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 152 in 11 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(80, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 153 in 10 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 80.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(80, 1)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 80 (reduce at GradientDescent.scala:150) finished in 0.012 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.017388 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 81 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 81 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 81 (MappedRDD[152] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 81 (MappedRDD[152] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 81.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 81.0:0 as TID 154 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 81.0:0 as 2398 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 81.0:1 as TID 155 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 81.0:1 as 2398 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 154
14/05/14 14:54:59 INFO executor.Executor: Running task ID 155
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 154 is 945
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 155 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 154 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Sending result for 155 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 154
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 155
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 154 in 8 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(81, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 155 in 8 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(81, 1)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 81.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 81 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.017909 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 82 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 82 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 82 (MappedRDD[154] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 82 (MappedRDD[154] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 82.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 82.0:0 as TID 156 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 82.0:0 as 2397 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 82.0:1 as TID 157 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 82.0:1 as 2397 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 156
14/05/14 14:54:59 INFO executor.Executor: Running task ID 157
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 157 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 157 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 157
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 156 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 156 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 156
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 157 in 8 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(82, 1)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 156 in 9 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(82, 0)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 82.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 82 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:54:59 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.015399 s
14/05/14 14:54:59 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Got job 83 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Final stage: Stage 83 (reduce at GradientDescent.scala:150)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting Stage 83 (MappedRDD[156] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 83 (MappedRDD[156] at map at GradientDescent.scala:145)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Adding task set 83.0 with 2 tasks
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 83.0:0 as TID 158 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 83.0:0 as 2394 bytes in 0 ms
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Starting task 83.0:1 as TID 159 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Serialized task 83.0:1 as 2394 bytes in 0 ms
14/05/14 14:54:59 INFO executor.Executor: Running task ID 159
14/05/14 14:54:59 INFO executor.Executor: Running task ID 158
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:54:59 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 158 is 945
14/05/14 14:54:59 INFO executor.Executor: Serialized size of result for 159 is 945
14/05/14 14:54:59 INFO executor.Executor: Sending result for 158 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Sending result for 159 directly to driver
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 158
14/05/14 14:54:59 INFO executor.Executor: Finished task ID 159
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 158 in 10 ms on localhost (progress: 1/2)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(83, 0)
14/05/14 14:54:59 INFO scheduler.TaskSetManager: Finished TID 159 in 11 ms on localhost (progress: 2/2)
14/05/14 14:54:59 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 83.0, whose tasks have all completed, from pool 
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Completed ResultTask(83, 1)
14/05/14 14:54:59 INFO scheduler.DAGScheduler: Stage 83 (reduce at GradientDescent.scala:150) finished in 0.013 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.018284 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 84 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 84 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 84 (MappedRDD[158] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 84 (MappedRDD[158] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 84.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 84.0:0 as TID 160 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 84.0:0 as 2386 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 84.0:1 as TID 161 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 84.0:1 as 2386 bytes in 1 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 160
14/05/14 14:55:00 INFO executor.Executor: Running task ID 161
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 161 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 160 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 161 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 160 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 160
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 161
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 161 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(84, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 160 in 9 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(84, 0)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 84.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 84 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.01708 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 85 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 85 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 85 (MappedRDD[160] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 85 (MappedRDD[160] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 85.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 85.0:0 as TID 162 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 85.0:0 as 2392 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 85.0:1 as TID 163 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 85.0:1 as 2392 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 162
14/05/14 14:55:00 INFO executor.Executor: Running task ID 163
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 162 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 163 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 162 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 163 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 162
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 163
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 163 in 10 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(85, 1)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(85, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 162 in 12 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 85.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 85 (reduce at GradientDescent.scala:150) finished in 0.012 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.01828 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 86 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 86 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 86 (MappedRDD[162] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 86 (MappedRDD[162] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 86.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 86.0:0 as TID 164 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 86.0:0 as 2384 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 86.0:1 as TID 165 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 86.0:1 as 2384 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 164
14/05/14 14:55:00 INFO executor.Executor: Running task ID 165
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 164 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 165 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 164 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 165 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 164
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 165
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 164 in 9 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(86, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 165 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(86, 1)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 86 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.016573 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 87 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 87 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 87 (MappedRDD[164] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 87 (MappedRDD[164] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 87.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 87.0:0 as TID 166 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 87.0:0 as 2392 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 87.0:1 as TID 167 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 87.0:1 as 2392 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 166
14/05/14 14:55:00 INFO executor.Executor: Running task ID 167
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 166 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 166 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 167 is 945
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 166
14/05/14 14:55:00 INFO executor.Executor: Sending result for 167 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 167
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(87, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 166 in 8 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 167 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(87, 1)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 87 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.015603 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 88 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 88 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 88 (MappedRDD[166] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 88 (MappedRDD[166] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 88.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 88.0:0 as TID 168 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 88.0:0 as 2384 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 88.0:1 as TID 169 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 88.0:1 as 2384 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 168
14/05/14 14:55:00 INFO executor.Executor: Running task ID 169
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 169 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 168 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 169 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 168 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 169
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 168
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 169 in 9 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(88, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 168 in 9 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(88, 0)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 88.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 88 (reduce at GradientDescent.scala:150) finished in 0.011 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.017447 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 89 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 89 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 89 (MappedRDD[168] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 89 (MappedRDD[168] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 89.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 89.0:0 as TID 170 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 89.0:0 as 2390 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 89.0:1 as TID 171 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 89.0:1 as 2390 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 170
14/05/14 14:55:00 INFO executor.Executor: Running task ID 171
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 171 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 170 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 171 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 170 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 171
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 170
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 171 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(89, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 170 in 7 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(89, 0)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 89 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.013867 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 90 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 90 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 90 (MappedRDD[170] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 90 (MappedRDD[170] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 90.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 90.0:0 as TID 172 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 90.0:0 as 2385 bytes in 1 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 90.0:1 as TID 173 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 90.0:1 as 2385 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 172
14/05/14 14:55:00 INFO executor.Executor: Running task ID 173
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 172 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 172 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 172
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 173 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 173 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 173
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(90, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 172 in 10 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 173 in 10 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(90, 1)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 90 (reduce at GradientDescent.scala:150) finished in 0.011 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.016814 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 91 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 91 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 91 (MappedRDD[172] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 91 (MappedRDD[172] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 91.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 91.0:0 as TID 174 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 91.0:0 as 2390 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 91.0:1 as TID 175 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 91.0:1 as 2390 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 174
14/05/14 14:55:00 INFO executor.Executor: Running task ID 175
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 174 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 174 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 174
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 175 is 945
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(91, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 174 in 8 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO executor.Executor: Sending result for 175 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 175
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 175 in 9 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(91, 1)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 91.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 91 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.014788 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 92 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 92 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 92 (MappedRDD[174] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 92 (MappedRDD[174] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 92.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 92.0:0 as TID 176 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 92.0:0 as 2386 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 92.0:1 as TID 177 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 92.0:1 as 2386 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 176
14/05/14 14:55:00 INFO executor.Executor: Running task ID 177
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 177 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 176 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 177 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 176 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 177
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 176
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 177 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(92, 1)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(92, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 176 in 9 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 92 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.014701 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 93 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 93 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 93 (MappedRDD[176] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 93 (MappedRDD[176] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 93.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 93.0:0 as TID 178 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 93.0:0 as 2391 bytes in 1 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 93.0:1 as TID 179 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 93.0:1 as 2391 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 178
14/05/14 14:55:00 INFO executor.Executor: Running task ID 179
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 179 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 178 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 179 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 178 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 178
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 179
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 179 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(93, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 178 in 9 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(93, 0)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 93 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.015661 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 94 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 94 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 94 (MappedRDD[178] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 94 (MappedRDD[178] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 94.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 94.0:0 as TID 180 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 94.0:0 as 2386 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 94.0:1 as TID 181 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 94.0:1 as 2386 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 180
14/05/14 14:55:00 INFO executor.Executor: Running task ID 181
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 181 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 180 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 181 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 180 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 181
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 180
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 181 in 8 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(94, 1)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(94, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 180 in 9 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 94.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 94 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.014785 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 95 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 95 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 95 (MappedRDD[180] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 95 (MappedRDD[180] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 95.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 95.0:0 as TID 182 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 95.0:0 as 2391 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 95.0:1 as TID 183 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 95.0:1 as 2391 bytes in 1 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 182
14/05/14 14:55:00 INFO executor.Executor: Running task ID 183
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 182 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 182 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 182
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 183 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 183 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 183
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 182 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(95, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 183 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(95, 1)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 95 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.018677 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 96 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 96 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 96 (MappedRDD[182] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 96 (MappedRDD[182] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 96.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 96.0:0 as TID 184 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 96.0:0 as 2385 bytes in 1 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 96.0:1 as TID 185 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 96.0:1 as 2385 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 184
14/05/14 14:55:00 INFO executor.Executor: Running task ID 185
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 185 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 184 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 185 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 184 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 185
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 184
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 185 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(96, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 184 in 9 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(96, 0)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 96.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 96 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.015245 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 97 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 97 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 97 (MappedRDD[184] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 97 (MappedRDD[184] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 97.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 97.0:0 as TID 186 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 97.0:0 as 2393 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 97.0:1 as TID 187 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 97.0:1 as 2393 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 186
14/05/14 14:55:00 INFO executor.Executor: Running task ID 187
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 186 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 187 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 187 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 186 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 187
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 186
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 187 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(97, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 186 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(97, 0)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 97.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 97 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.014963 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 98 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 98 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 98 (MappedRDD[186] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 98 (MappedRDD[186] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 98.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 98.0:0 as TID 188 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 98.0:0 as 2426 bytes in 1 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 98.0:1 as TID 189 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 98.0:1 as 2426 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 188
14/05/14 14:55:00 INFO executor.Executor: Running task ID 189
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 188 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 188 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 189 is 945
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 188
14/05/14 14:55:00 INFO executor.Executor: Sending result for 189 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 189
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 188 in 8 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(98, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 189 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(98, 1)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 98.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 98 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.01394 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 99 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 99 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 99 (MappedRDD[188] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 99 (MappedRDD[188] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 99.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 99.0:0 as TID 190 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 99.0:0 as 2404 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 99.0:1 as TID 191 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 99.0:1 as 2404 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 190
14/05/14 14:55:00 INFO executor.Executor: Running task ID 191
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 190 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 190 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 191 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 191 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 190
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 191
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 190 in 9 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(99, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 191 in 9 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(99, 1)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 99.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 99 (reduce at GradientDescent.scala:150) finished in 0.011 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.015869 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 100 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 100 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 100 (MappedRDD[190] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 100 (MappedRDD[190] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 100.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 100.0:0 as TID 192 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 100.0:0 as 2412 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 100.0:1 as TID 193 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 100.0:1 as 2412 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 192
14/05/14 14:55:00 INFO executor.Executor: Running task ID 193
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 192 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 192 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 192
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 193 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 193 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 193
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 192 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(100, 0)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(100, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 193 in 7 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 100.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 100 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.014149 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 101 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 101 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 101 (MappedRDD[192] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 101 (MappedRDD[192] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 101.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 101.0:0 as TID 194 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 101.0:0 as 2402 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 101.0:1 as TID 195 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 101.0:1 as 2402 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 194
14/05/14 14:55:00 INFO executor.Executor: Running task ID 195
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 194 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 195 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 194 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 195 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 194
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 195
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 194 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(101, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 195 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(101, 1)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 101.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 101 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.014101 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 102 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 102 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 102 (MappedRDD[194] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 102 (MappedRDD[194] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 102.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 102.0:0 as TID 196 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 102.0:0 as 2397 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 102.0:1 as TID 197 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 102.0:1 as 2397 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 196
14/05/14 14:55:00 INFO executor.Executor: Running task ID 197
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 196 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 196 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 197 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 197 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 197
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 196
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 196 in 8 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(102, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 197 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(102, 1)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 102.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 102 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.013866 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 103 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 103 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 103 (MappedRDD[196] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 103 (MappedRDD[196] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 103.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 103.0:0 as TID 198 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 103.0:0 as 2394 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 103.0:1 as TID 199 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 103.0:1 as 2394 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 199
14/05/14 14:55:00 INFO executor.Executor: Running task ID 198
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 199 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 198 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 199 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 198 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 199
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 198
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 199 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(103, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 198 in 9 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(103, 0)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 103.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 103 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.015228 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 104 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 104 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 104 (MappedRDD[198] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 104 (MappedRDD[198] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 104.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 104.0:0 as TID 200 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 104.0:0 as 2384 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 104.0:1 as TID 201 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 104.0:1 as 2384 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 200
14/05/14 14:55:00 INFO executor.Executor: Running task ID 201
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 201 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 201 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 201
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 201 in 11 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(104, 1)
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 200 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 200 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 200
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 200 in 13 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 104.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(104, 0)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 104 (reduce at GradientDescent.scala:150) finished in 0.013 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.01857 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 105 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 105 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 105 (MappedRDD[200] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 105 (MappedRDD[200] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 105.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 105.0:0 as TID 202 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 105.0:0 as 2387 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 105.0:1 as TID 203 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 105.0:1 as 2387 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 202
14/05/14 14:55:00 INFO executor.Executor: Running task ID 203
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 202 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 203 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 203 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 202 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 203
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 202
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 203 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(105, 1)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(105, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 202 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 105.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 105 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.013494 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 106 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 106 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 106 (MappedRDD[202] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 106 (MappedRDD[202] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 106.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 106.0:0 as TID 204 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 106.0:0 as 2394 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 106.0:1 as TID 205 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 106.0:1 as 2394 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 204
14/05/14 14:55:00 INFO executor.Executor: Running task ID 205
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 204 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 204 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 204
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 205 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 205 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 205
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 204 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(106, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 205 in 7 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(106, 1)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 106.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 106 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.013898 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 107 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 107 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 107 (MappedRDD[204] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 107 (MappedRDD[204] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 107.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 107.0:0 as TID 206 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 107.0:0 as 2386 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 107.0:1 as TID 207 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 107.0:1 as 2386 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 206
14/05/14 14:55:00 INFO executor.Executor: Running task ID 207
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 206 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 207 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 206 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 207 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 207
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 206
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 206 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(107, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 207 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 107.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(107, 1)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 107 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.013216 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 108 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 108 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 108 (MappedRDD[206] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 108 (MappedRDD[206] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 108.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 108.0:0 as TID 208 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 108.0:0 as 2392 bytes in 1 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 108.0:1 as TID 209 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 108.0:1 as 2392 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 209
14/05/14 14:55:00 INFO executor.Executor: Running task ID 208
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 208 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 208 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 208
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 208 in 8 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(108, 0)
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 209 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 209 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 209
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 209 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 108.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(108, 1)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 108 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.016314 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 109 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 109 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 109 (MappedRDD[208] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 109 (MappedRDD[208] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 109.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 109.0:0 as TID 210 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 109.0:0 as 2383 bytes in 1 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 109.0:1 as TID 211 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 109.0:1 as 2383 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 210
14/05/14 14:55:00 INFO executor.Executor: Running task ID 211
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 210 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 211 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 210 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 211 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 210
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 211
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 210 in 10 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(109, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 211 in 10 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 109.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(109, 1)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 109 (reduce at GradientDescent.scala:150) finished in 0.011 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.016753 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 110 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 110 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 110 (MappedRDD[210] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 110 (MappedRDD[210] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 110.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 110.0:0 as TID 212 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 110.0:0 as 2391 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 110.0:1 as TID 213 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 110.0:1 as 2391 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 212
14/05/14 14:55:00 INFO executor.Executor: Running task ID 213
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 212 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 212 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 212
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 212 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(110, 0)
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 213 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 213 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 213
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 213 in 7 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 110.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(110, 1)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 110 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.012657 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 111 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 111 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 111 (MappedRDD[212] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 111 (MappedRDD[212] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 111.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 111.0:0 as TID 214 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 111.0:0 as 2387 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 111.0:1 as TID 215 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 111.0:1 as 2387 bytes in 1 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 214
14/05/14 14:55:00 INFO executor.Executor: Running task ID 215
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 214 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 214 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 214
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 215 is 945
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(111, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 214 in 8 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO executor.Executor: Sending result for 215 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 215
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 215 in 7 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(111, 1)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 111.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 111 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.01375 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 112 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 112 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 112 (MappedRDD[214] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 112 (MappedRDD[214] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 112.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 112.0:0 as TID 216 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 112.0:0 as 2391 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 112.0:1 as TID 217 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 112.0:1 as 2391 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 216
14/05/14 14:55:00 INFO executor.Executor: Running task ID 217
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 216 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 216 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 216
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 216 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(112, 0)
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 217 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 217 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 217
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 217 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(112, 1)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 112.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 112 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.013121 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 113 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 113 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 113 (MappedRDD[216] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 113 (MappedRDD[216] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 113.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 113.0:0 as TID 218 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 113.0:0 as 2414 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 113.0:1 as TID 219 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 113.0:1 as 2414 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 218
14/05/14 14:55:00 INFO executor.Executor: Running task ID 219
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 218 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 219 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 218 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 219 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 218
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 219
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 218 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(113, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 219 in 7 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(113, 1)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 113.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 113 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.012934 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 114 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 114 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 114 (MappedRDD[218] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 114 (MappedRDD[218] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 114.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 114.0:0 as TID 220 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 114.0:0 as 2392 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 114.0:1 as TID 221 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 114.0:1 as 2392 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 221
14/05/14 14:55:00 INFO executor.Executor: Running task ID 220
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 220 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 221 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 220 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 220
14/05/14 14:55:00 INFO executor.Executor: Sending result for 221 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 221
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 220 in 11 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(114, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 221 in 11 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(114, 1)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 114.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 114 (reduce at GradientDescent.scala:150) finished in 0.013 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.017541 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 115 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 115 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 115 (MappedRDD[220] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 115 (MappedRDD[220] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 115.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 115.0:0 as TID 222 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 115.0:0 as 2401 bytes in 1 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 115.0:1 as TID 223 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 115.0:1 as 2401 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 223
14/05/14 14:55:00 INFO executor.Executor: Running task ID 222
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 223 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 223 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 223
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 222 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 222 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 222
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 223 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(115, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 222 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(115, 0)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 115.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 115 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.012412 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 116 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 116 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 116 (MappedRDD[222] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 116 (MappedRDD[222] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 116.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 116.0:0 as TID 224 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 116.0:0 as 2400 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 116.0:1 as TID 225 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 116.0:1 as 2400 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 224
14/05/14 14:55:00 INFO executor.Executor: Running task ID 225
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 225 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 225 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 225
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 224 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 224 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 224
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(116, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 225 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 224 in 9 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(116, 0)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 116.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 116 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.012963 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 117 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 117 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 117 (MappedRDD[224] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 117 (MappedRDD[224] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 117.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 117.0:0 as TID 226 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 117.0:0 as 2396 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 117.0:1 as TID 227 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 117.0:1 as 2396 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 226
14/05/14 14:55:00 INFO executor.Executor: Running task ID 227
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 227 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 226 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 227 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 226 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 226
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 227
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 227 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(117, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 226 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(117, 0)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 117.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 117 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.013144 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 118 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 118 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 118 (MappedRDD[226] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 118 (MappedRDD[226] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 118.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 118.0:0 as TID 228 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 118.0:0 as 2385 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 118.0:1 as TID 229 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 118.0:1 as 2385 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 228
14/05/14 14:55:00 INFO executor.Executor: Running task ID 229
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 229 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 229 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 229
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 228 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 228 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 228
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(118, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 229 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 228 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(118, 0)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 118.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 118 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.01 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 119 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 119 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 119 (MappedRDD[228] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 119 (MappedRDD[228] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 119.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 119.0:0 as TID 230 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 119.0:0 as 2392 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 119.0:1 as TID 231 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 119.0:1 as 2392 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 230
14/05/14 14:55:00 INFO executor.Executor: Running task ID 231
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 230 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 230 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 230
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 230 in 8 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(119, 0)
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 231 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 231 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 231
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(119, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 231 in 9 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 119.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 119 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.013486 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 120 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 120 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 120 (MappedRDD[230] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 120 (MappedRDD[230] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 120.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 120.0:0 as TID 232 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 120.0:0 as 2387 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 120.0:1 as TID 233 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 120.0:1 as 2387 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 232
14/05/14 14:55:00 INFO executor.Executor: Running task ID 233
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 233 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 233 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 232 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 232 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 232
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 233
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 233 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(120, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 232 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(120, 0)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 120.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 120 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.012159 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 121 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 121 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 121 (MappedRDD[232] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 121 (MappedRDD[232] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 121.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 121.0:0 as TID 234 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 121.0:0 as 2391 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 121.0:1 as TID 235 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 121.0:1 as 2391 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 234
14/05/14 14:55:00 INFO executor.Executor: Running task ID 235
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 234 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 234 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 234
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 234 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(121, 0)
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 235 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 235 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 235
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 235 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 121.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(121, 1)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 121 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.011673 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 122 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 122 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 122 (MappedRDD[234] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 122 (MappedRDD[234] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 122.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 122.0:0 as TID 236 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 122.0:0 as 2385 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 122.0:1 as TID 237 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 122.0:1 as 2385 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 236
14/05/14 14:55:00 INFO executor.Executor: Running task ID 237
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 237 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 236 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 237 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 236 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 236
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 237
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 237 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(122, 1)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(122, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 236 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 122.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 122 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.017804 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 123 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 123 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 123 (MappedRDD[236] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 123 (MappedRDD[236] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 123.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 123.0:0 as TID 238 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 123.0:0 as 2393 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 123.0:1 as TID 239 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 123.0:1 as 2393 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 238
14/05/14 14:55:00 INFO executor.Executor: Running task ID 239
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 238 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 238 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 238
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 238 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(123, 0)
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 239 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 239 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 239
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(123, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 239 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 123.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 123 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010823 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 124 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 124 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 124 (MappedRDD[238] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 124 (MappedRDD[238] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 124.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 124.0:0 as TID 240 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 124.0:0 as 2383 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 124.0:1 as TID 241 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 124.0:1 as 2383 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 240
14/05/14 14:55:00 INFO executor.Executor: Running task ID 241
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 240 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 240 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 241 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 241 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 240
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 241
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 240 in 9 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(124, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 241 in 10 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(124, 1)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 124.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 124 (reduce at GradientDescent.scala:150) finished in 0.011 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.01467 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 125 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 125 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 125 (MappedRDD[240] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 125 (MappedRDD[240] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 125.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 125.0:0 as TID 242 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 125.0:0 as 2432 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 125.0:1 as TID 243 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 125.0:1 as 2432 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 242
14/05/14 14:55:00 INFO executor.Executor: Running task ID 243
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 242 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 242 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 242
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 242 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(125, 0)
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 243 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 243 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 243
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 243 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 125.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(125, 1)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 125 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.012394 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 126 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 126 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 126 (MappedRDD[242] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 126 (MappedRDD[242] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 126.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 126.0:0 as TID 244 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 126.0:0 as 2394 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 126.0:1 as TID 245 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 126.0:1 as 2394 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 245
14/05/14 14:55:00 INFO executor.Executor: Running task ID 244
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 245 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 245 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 245
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 244 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 244 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 244
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 245 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(126, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 244 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(126, 0)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 126.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 126 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009373 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 127 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 127 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 127 (MappedRDD[244] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 127 (MappedRDD[244] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 127.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 127.0:0 as TID 246 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 127.0:0 as 2400 bytes in 1 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 127.0:1 as TID 247 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 127.0:1 as 2400 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 247
14/05/14 14:55:00 INFO executor.Executor: Running task ID 246
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 246 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 246 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 246
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 246 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(127, 0)
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 247 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 247 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 247
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 247 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(127, 1)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 127.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 127 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010934 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 128 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 128 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 128 (MappedRDD[246] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 128 (MappedRDD[246] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 128.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 128.0:0 as TID 248 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 128.0:0 as 2395 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 128.0:1 as TID 249 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 128.0:1 as 2395 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 248
14/05/14 14:55:00 INFO executor.Executor: Running task ID 249
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 248 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 248 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 248
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 249 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 249 directly to driver
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 248 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 249
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(128, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 249 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 128.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(128, 1)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 128 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010332 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 129 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 129 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 129 (MappedRDD[248] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 129 (MappedRDD[248] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 129.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 129.0:0 as TID 250 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 129.0:0 as 2395 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 129.0:1 as TID 251 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 129.0:1 as 2395 bytes in 1 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 250
14/05/14 14:55:00 INFO executor.Executor: Running task ID 251
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 250 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 250 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 250
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 251 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 251 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 251
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(129, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 250 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 251 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 129.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(129, 1)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 129 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.012271 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 130 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 130 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 130 (MappedRDD[250] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 130 (MappedRDD[250] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 130.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 130.0:0 as TID 252 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 130.0:0 as 2397 bytes in 1 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 130.0:1 as TID 253 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 130.0:1 as 2397 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 252
14/05/14 14:55:00 INFO executor.Executor: Running task ID 253
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 253 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 253 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 253
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 252 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 252 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 252
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(130, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 253 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(130, 0)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 252 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 130.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 130 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010392 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 131 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 131 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 131 (MappedRDD[252] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 131 (MappedRDD[252] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 131.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 131.0:0 as TID 254 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 131.0:0 as 2385 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 131.0:1 as TID 255 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 131.0:1 as 2385 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 254
14/05/14 14:55:00 INFO executor.Executor: Running task ID 255
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 254 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 254 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 255 is 945
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 254
14/05/14 14:55:00 INFO executor.Executor: Sending result for 255 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 255
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 254 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(131, 0)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(131, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 255 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 131.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 131 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.011015 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 132 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 132 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 132 (MappedRDD[254] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 132 (MappedRDD[254] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 132.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 132.0:0 as TID 256 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 132.0:0 as 2385 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 132.0:1 as TID 257 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 132.0:1 as 2385 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 256
14/05/14 14:55:00 INFO executor.Executor: Running task ID 257
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 256 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 256 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 256
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 256 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(132, 0)
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 257 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 257 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 257
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(132, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 257 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 132.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 132 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010524 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 133 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 133 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 133 (MappedRDD[256] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 133 (MappedRDD[256] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 133.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 133.0:0 as TID 258 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 133.0:0 as 2391 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 133.0:1 as TID 259 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 133.0:1 as 2391 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 258
14/05/14 14:55:00 INFO executor.Executor: Running task ID 259
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 258 is 945
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 259 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 259 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Sending result for 258 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 259
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 258
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(133, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 259 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 258 in 4 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 133.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(133, 0)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 133 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008839 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 134 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 134 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 134 (MappedRDD[258] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 134 (MappedRDD[258] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 134.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 134.0:0 as TID 260 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 134.0:0 as 2384 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 134.0:1 as TID 261 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 134.0:1 as 2384 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 260
14/05/14 14:55:00 INFO executor.Executor: Running task ID 261
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:00 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 260 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 260 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 260
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 260 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(134, 0)
14/05/14 14:55:00 INFO executor.Executor: Serialized size of result for 261 is 945
14/05/14 14:55:00 INFO executor.Executor: Sending result for 261 directly to driver
14/05/14 14:55:00 INFO executor.Executor: Finished task ID 261
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Completed ResultTask(134, 1)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Finished TID 261 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 134.0, whose tasks have all completed, from pool 
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Stage 134 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:00 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.011348 s
14/05/14 14:55:00 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Got job 135 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Final stage: Stage 135 (reduce at GradientDescent.scala:150)
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting Stage 135 (MappedRDD[260] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:00 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 135 (MappedRDD[260] at map at GradientDescent.scala:145)
14/05/14 14:55:00 INFO scheduler.TaskSchedulerImpl: Adding task set 135.0 with 2 tasks
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 135.0:0 as TID 262 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 135.0:0 as 2391 bytes in 0 ms
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Starting task 135.0:1 as TID 263 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:00 INFO scheduler.TaskSetManager: Serialized task 135.0:1 as 2391 bytes in 0 ms
14/05/14 14:55:00 INFO executor.Executor: Running task ID 262
14/05/14 14:55:00 INFO executor.Executor: Running task ID 263
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:00 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 263 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 263 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 262 is 945
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 263
14/05/14 14:55:01 INFO executor.Executor: Sending result for 262 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 262
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 263 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(135, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(135, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 262 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 135.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 135 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009957 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 136 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 136 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 136 (MappedRDD[262] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 136 (MappedRDD[262] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 136.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 136.0:0 as TID 264 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 136.0:0 as 2385 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 136.0:1 as TID 265 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 136.0:1 as 2385 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 264
14/05/14 14:55:01 INFO executor.Executor: Running task ID 265
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 265 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 265 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 265
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 265 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(136, 1)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 264 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 264 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 264
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 264 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(136, 0)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 136.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 136 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009597 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 137 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 137 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 137 (MappedRDD[264] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 137 (MappedRDD[264] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 137.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 137.0:0 as TID 266 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 137.0:0 as 2391 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 137.0:1 as TID 267 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 137.0:1 as 2391 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 266
14/05/14 14:55:01 INFO executor.Executor: Running task ID 267
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 267 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 267 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 267
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 266 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 266 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 266
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 267 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(137, 1)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 266 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(137, 0)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 137.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 137 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.012268 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 138 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 138 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 138 (MappedRDD[266] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 138 (MappedRDD[266] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 138.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 138.0:0 as TID 268 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 138.0:0 as 2386 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 138.0:1 as TID 269 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 138.0:1 as 2386 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 268
14/05/14 14:55:01 INFO executor.Executor: Running task ID 269
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 268 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 268 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 268
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(138, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 268 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 269 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 269 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 269
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 269 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(138, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 138.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 138 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010976 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 139 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 139 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 139 (MappedRDD[268] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 139 (MappedRDD[268] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 139.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 139.0:0 as TID 270 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 139.0:0 as 2392 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 139.0:1 as TID 271 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 139.0:1 as 2392 bytes in 1 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 270
14/05/14 14:55:01 INFO executor.Executor: Running task ID 271
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 270 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 270 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 270
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 271 is 945
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(139, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 270 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO executor.Executor: Sending result for 271 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 271
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 271 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 139.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(139, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 139 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010313 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 140 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 140 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 140 (MappedRDD[270] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 140 (MappedRDD[270] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 140.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 140.0:0 as TID 272 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 140.0:0 as 2433 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 140.0:1 as TID 273 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 140.0:1 as 2433 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 272
14/05/14 14:55:01 INFO executor.Executor: Running task ID 273
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 272 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 272 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 272
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 273 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 273 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 273
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 272 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(140, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 273 in 4 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(140, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 140.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 140 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008915 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 141 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 141 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 141 (MappedRDD[272] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 141 (MappedRDD[272] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 141.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 141.0:0 as TID 274 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 141.0:0 as 2397 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 141.0:1 as TID 275 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 141.0:1 as 2397 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 275
14/05/14 14:55:01 INFO executor.Executor: Running task ID 274
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 274 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 274 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 274
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 274 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(141, 0)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 275 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 275 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 275
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 275 in 7 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 141.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(141, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 141 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.011451 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 142 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 142 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 142 (MappedRDD[274] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 142 (MappedRDD[274] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 142.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 142.0:0 as TID 276 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 142.0:0 as 2401 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 142.0:1 as TID 277 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 142.0:1 as 2401 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 277
14/05/14 14:55:01 INFO executor.Executor: Running task ID 276
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 277 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 277 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 276 is 945
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 277
14/05/14 14:55:01 INFO executor.Executor: Sending result for 276 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 276
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 277 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(142, 1)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 276 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(142, 0)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 142.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 142 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008793 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 143 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 143 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 143 (MappedRDD[276] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 143 (MappedRDD[276] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 143.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 143.0:0 as TID 278 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 143.0:0 as 2399 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 143.0:1 as TID 279 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 143.0:1 as 2399 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 278
14/05/14 14:55:01 INFO executor.Executor: Running task ID 279
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 278 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 278 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 278
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(143, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 278 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 279 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 279 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 279
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(143, 1)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 279 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 143.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 143 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.011627 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 144 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 144 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 144 (MappedRDD[278] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 144 (MappedRDD[278] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 144.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 144.0:0 as TID 280 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 144.0:0 as 2397 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 144.0:1 as TID 281 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 144.0:1 as 2397 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 280
14/05/14 14:55:01 INFO executor.Executor: Running task ID 281
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 280 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 280 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 281 is 945
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 280
14/05/14 14:55:01 INFO executor.Executor: Sending result for 281 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 281
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 280 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(144, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 281 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(144, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 144.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 144 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.011855 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 145 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 145 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 145 (MappedRDD[280] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 145 (MappedRDD[280] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 145.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 145.0:0 as TID 282 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 145.0:0 as 2397 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 145.0:1 as TID 283 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 145.0:1 as 2397 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 282
14/05/14 14:55:01 INFO executor.Executor: Running task ID 283
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 282 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 282 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 282
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 283 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 283 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 283
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 282 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(145, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 283 in 4 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(145, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 145.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 145 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008845 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 146 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 146 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 146 (MappedRDD[282] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 146 (MappedRDD[282] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 146.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 146.0:0 as TID 284 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 146.0:0 as 2396 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 146.0:1 as TID 285 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 146.0:1 as 2396 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 284
14/05/14 14:55:01 INFO executor.Executor: Running task ID 285
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 285 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 285 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 285
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 285 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(146, 1)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 284 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 284 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 284
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 284 in 7 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 146.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(146, 0)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 146 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.011186 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 147 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 147 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 147 (MappedRDD[284] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 147 (MappedRDD[284] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 147.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 147.0:0 as TID 286 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 147.0:0 as 2385 bytes in 1 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 147.0:1 as TID 287 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 147.0:1 as 2385 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 286
14/05/14 14:55:01 INFO executor.Executor: Running task ID 287
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 286 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 286 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 286
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 286 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(147, 0)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 287 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 287 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 287
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 287 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 147.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(147, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 147 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010388 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 148 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 148 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 148 (MappedRDD[286] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 148 (MappedRDD[286] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 148.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 148.0:0 as TID 288 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 148.0:0 as 2389 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 148.0:1 as TID 289 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 148.0:1 as 2389 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 289
14/05/14 14:55:01 INFO executor.Executor: Running task ID 288
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 288 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 288 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 288
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 289 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 289 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 289
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 288 in 8 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(148, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 289 in 9 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 148.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(148, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 148 (reduce at GradientDescent.scala:150) finished in 0.010 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.013608 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 149 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 149 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 149 (MappedRDD[288] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 149 (MappedRDD[288] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 149.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 149.0:0 as TID 290 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 149.0:0 as 2388 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 149.0:1 as TID 291 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 149.0:1 as 2388 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 291
14/05/14 14:55:01 INFO executor.Executor: Running task ID 290
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 291 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 291 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 291
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(149, 1)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 291 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 290 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 290 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 290
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 290 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(149, 0)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 149.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 149 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010628 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 150 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 150 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 150 (MappedRDD[290] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 150 (MappedRDD[290] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 150.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 150.0:0 as TID 292 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 150.0:0 as 2392 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 150.0:1 as TID 293 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 150.0:1 as 2392 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 293
14/05/14 14:55:01 INFO executor.Executor: Running task ID 292
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 292 is 945
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 293 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 292 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Sending result for 293 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 292
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 293
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(150, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 292 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 293 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 150.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(150, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 150 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.00909 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 151 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 151 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 151 (MappedRDD[292] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 151 (MappedRDD[292] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 151.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 151.0:0 as TID 294 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 151.0:0 as 2384 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 151.0:1 as TID 295 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 151.0:1 as 2384 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 294
14/05/14 14:55:01 INFO executor.Executor: Running task ID 295
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 295 is 945
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 294 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 295 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Sending result for 294 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 295
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 294
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 295 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(151, 1)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 294 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(151, 0)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 151.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 151 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009257 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 152 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 152 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 152 (MappedRDD[294] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 152 (MappedRDD[294] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 152.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 152.0:0 as TID 296 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 152.0:0 as 2392 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 152.0:1 as TID 297 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 152.0:1 as 2392 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 296
14/05/14 14:55:01 INFO executor.Executor: Running task ID 297
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 297 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 297 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 297
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 296 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 296 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 296
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(152, 1)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 297 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 296 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 152.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(152, 0)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 152 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010636 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 153 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 153 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 153 (MappedRDD[296] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 153 (MappedRDD[296] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 153.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 153.0:0 as TID 298 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 153.0:0 as 2385 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 153.0:1 as TID 299 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 153.0:1 as 2385 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 298
14/05/14 14:55:01 INFO executor.Executor: Running task ID 299
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 299 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 299 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 299
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 299 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(153, 1)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 298 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 298 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 298
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 298 in 7 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(153, 0)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 153.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 153 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.011544 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 154 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 154 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 154 (MappedRDD[298] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 154 (MappedRDD[298] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 154.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 154.0:0 as TID 300 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 154.0:0 as 2389 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 154.0:1 as TID 301 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 154.0:1 as 2389 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 300
14/05/14 14:55:01 INFO executor.Executor: Running task ID 301
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 300 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 300 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 300
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(154, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 300 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 301 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 301 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 301
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 301 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 154.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(154, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 154 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010731 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 155 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 155 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 155 (MappedRDD[300] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 155 (MappedRDD[300] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 155.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 155.0:0 as TID 302 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 155.0:0 as 2435 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 155.0:1 as TID 303 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 155.0:1 as 2435 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 303
14/05/14 14:55:01 INFO executor.Executor: Running task ID 302
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 302 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 302 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 302
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 302 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(155, 0)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 303 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 303 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 303
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 303 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(155, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 155.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 155 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010027 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 156 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 156 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 156 (MappedRDD[302] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 156 (MappedRDD[302] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 156.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 156.0:0 as TID 304 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 156.0:0 as 2402 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 156.0:1 as TID 305 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 156.0:1 as 2402 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 305
14/05/14 14:55:01 INFO executor.Executor: Running task ID 304
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 304 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 304 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 304
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(156, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 304 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 305 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 305 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 305
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 305 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(156, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 156.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 156 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010316 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 157 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 157 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 157 (MappedRDD[304] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 157 (MappedRDD[304] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 157.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 157.0:0 as TID 306 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 157.0:0 as 2401 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 157.0:1 as TID 307 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 157.0:1 as 2401 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 306
14/05/14 14:55:01 INFO executor.Executor: Running task ID 307
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 306 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 306 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 306
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 307 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 307 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 307
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 306 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(157, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 307 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(157, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 157.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 157 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010108 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 158 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 158 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 158 (MappedRDD[306] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 158 (MappedRDD[306] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 158.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 158.0:0 as TID 308 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 158.0:0 as 2401 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 158.0:1 as TID 309 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 158.0:1 as 2401 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 309
14/05/14 14:55:01 INFO executor.Executor: Running task ID 308
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 309 is 945
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 308 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 308 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Sending result for 309 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 308
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 309
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 308 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(158, 0)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(158, 1)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 309 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 158 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 158.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010114 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 159 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 159 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 159 (MappedRDD[308] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 159 (MappedRDD[308] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 159.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 159.0:0 as TID 310 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 159.0:0 as 2395 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 159.0:1 as TID 311 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 159.0:1 as 2395 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 310
14/05/14 14:55:01 INFO executor.Executor: Running task ID 311
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 310 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 310 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 310
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 311 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 311 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 311
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 310 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(159, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 311 in 4 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 159.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(159, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 159 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008424 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 160 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 160 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 160 (MappedRDD[310] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 160 (MappedRDD[310] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 160.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 160.0:0 as TID 312 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 160.0:0 as 2397 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 160.0:1 as TID 313 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 160.0:1 as 2397 bytes in 1 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 312
14/05/14 14:55:01 INFO executor.Executor: Running task ID 313
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 312 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 312 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 312
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 312 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(160, 0)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 313 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 313 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 313
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 313 in 7 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 160.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(160, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 160 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.011829 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 161 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 161 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 161 (MappedRDD[312] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 161 (MappedRDD[312] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 161.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 161.0:0 as TID 314 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 161.0:0 as 2397 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 161.0:1 as TID 315 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 161.0:1 as 2397 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 314
14/05/14 14:55:01 INFO executor.Executor: Running task ID 315
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 315 is 945
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 314 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 314 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Sending result for 315 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 314
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 315
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 314 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(161, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 315 in 4 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(161, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 161.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 161 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009573 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 162 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 162 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 162 (MappedRDD[314] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 162 (MappedRDD[314] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 162.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 162.0:0 as TID 316 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 162.0:0 as 2386 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 162.0:1 as TID 317 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 162.0:1 as 2386 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 317
14/05/14 14:55:01 INFO executor.Executor: Running task ID 316
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 317 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 317 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 317
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 317 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(162, 1)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 316 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 316 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 316
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 316 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(162, 0)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 162.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 162 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.01001 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 163 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 163 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 163 (MappedRDD[316] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 163 (MappedRDD[316] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 163.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 163.0:0 as TID 318 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 163.0:0 as 2388 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 163.0:1 as TID 319 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 163.0:1 as 2388 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 318
14/05/14 14:55:01 INFO executor.Executor: Running task ID 319
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 319 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 319 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 319
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 318 is 945
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 319 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(163, 1)
14/05/14 14:55:01 INFO executor.Executor: Sending result for 318 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 318
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 318 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 163.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(163, 0)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 163 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.011236 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 164 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 164 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 164 (MappedRDD[318] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 164 (MappedRDD[318] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 164.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 164.0:0 as TID 320 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 164.0:0 as 2386 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 164.0:1 as TID 321 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 164.0:1 as 2386 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 320
14/05/14 14:55:01 INFO executor.Executor: Running task ID 321
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 321 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 321 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 321
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 321 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(164, 1)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 320 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 320 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 320
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 320 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 164.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(164, 0)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 164 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010036 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 165 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 165 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 165 (MappedRDD[320] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 165 (MappedRDD[320] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 165.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 165.0:0 as TID 322 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 165.0:0 as 2391 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 165.0:1 as TID 323 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 165.0:1 as 2391 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 322
14/05/14 14:55:01 INFO executor.Executor: Running task ID 323
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 322 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 322 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 322
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 323 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 323 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 323
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 322 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(165, 0)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(165, 1)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 323 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 165.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 165 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009054 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 166 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 166 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 166 (MappedRDD[322] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 166 (MappedRDD[322] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 166.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 166.0:0 as TID 324 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 166.0:0 as 2384 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 166.0:1 as TID 325 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 166.0:1 as 2384 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 324
14/05/14 14:55:01 INFO executor.Executor: Running task ID 325
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 325 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 325 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 325
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 324 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 324 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 324
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(166, 1)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 325 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(166, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 324 in 7 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 166.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 166 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.012046 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 167 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 167 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 167 (MappedRDD[324] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 167 (MappedRDD[324] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 167.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 167.0:0 as TID 326 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 167.0:0 as 2391 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 167.0:1 as TID 327 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 167.0:1 as 2391 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 326
14/05/14 14:55:01 INFO executor.Executor: Running task ID 327
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 326 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 326 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 326
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 326 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(167, 0)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 327 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 327 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 327
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 327 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 167.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(167, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 167 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009699 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 168 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 168 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 168 (MappedRDD[326] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 168 (MappedRDD[326] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 168.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 168.0:0 as TID 328 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 168.0:0 as 2384 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 168.0:1 as TID 329 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 168.0:1 as 2384 bytes in 1 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 329
14/05/14 14:55:01 INFO executor.Executor: Running task ID 328
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 329 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 329 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 328 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 328 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 329
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 328
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 329 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(168, 1)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 328 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(168, 0)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 168.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 168 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009502 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 169 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 169 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 169 (MappedRDD[328] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 169 (MappedRDD[328] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 169.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 169.0:0 as TID 330 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 169.0:0 as 2393 bytes in 1 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 169.0:1 as TID 331 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 169.0:1 as 2393 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 330
14/05/14 14:55:01 INFO executor.Executor: Running task ID 331
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 331 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 331 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 331
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 331 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 330 is 945
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(169, 1)
14/05/14 14:55:01 INFO executor.Executor: Sending result for 330 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 330
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 330 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 169.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(169, 0)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 169 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.0112 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 170 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 170 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 170 (MappedRDD[330] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 170 (MappedRDD[330] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 170.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 170.0:0 as TID 332 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 170.0:0 as 2434 bytes in 1 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 170.0:1 as TID 333 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 170.0:1 as 2434 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 332
14/05/14 14:55:01 INFO executor.Executor: Running task ID 333
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 333 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 333 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 333
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 332 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 332 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 332
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 333 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(170, 1)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 332 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 170.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(170, 0)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 170 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009629 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 171 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 171 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 171 (MappedRDD[332] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 171 (MappedRDD[332] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 171.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 171.0:0 as TID 334 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 171.0:0 as 2400 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 171.0:1 as TID 335 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 171.0:1 as 2400 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 334
14/05/14 14:55:01 INFO executor.Executor: Running task ID 335
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 334 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 334 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 334
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 335 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 335 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 335
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 334 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(171, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 335 in 4 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(171, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 171.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 171 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.00817 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 172 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 172 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 172 (MappedRDD[334] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 172 (MappedRDD[334] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 172.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 172.0:0 as TID 336 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 172.0:0 as 2404 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 172.0:1 as TID 337 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 172.0:1 as 2404 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 336
14/05/14 14:55:01 INFO executor.Executor: Running task ID 337
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 336 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 336 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 336
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 336 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(172, 0)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 337 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 337 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 337
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 337 in 7 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 172.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(172, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 172 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.011382 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 173 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 173 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 173 (MappedRDD[336] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 173 (MappedRDD[336] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 173.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 173.0:0 as TID 338 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 173.0:0 as 2399 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 173.0:1 as TID 339 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 173.0:1 as 2399 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 338
14/05/14 14:55:01 INFO executor.Executor: Running task ID 339
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 338 is 945
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 339 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 338 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Sending result for 339 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 338
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 339
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 338 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(173, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 339 in 4 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(173, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 173.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 173 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008245 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 174 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 174 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 174 (MappedRDD[338] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 174 (MappedRDD[338] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 174.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 174.0:0 as TID 340 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 174.0:0 as 2393 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 174.0:1 as TID 341 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 174.0:1 as 2393 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 340
14/05/14 14:55:01 INFO executor.Executor: Running task ID 341
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 340 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 340 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 340
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 341 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 341 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 341
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 340 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(174, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 341 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(174, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 174.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 174 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010888 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 175 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 175 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 175 (MappedRDD[340] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 175 (MappedRDD[340] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 175.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 175.0:0 as TID 342 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 175.0:0 as 2395 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 175.0:1 as TID 343 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 175.0:1 as 2395 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 342
14/05/14 14:55:01 INFO executor.Executor: Running task ID 343
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 342 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 342 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 343 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 343 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 342
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 343
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(175, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 342 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 343 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(175, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 175.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 175 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010867 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 176 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 176 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 176 (MappedRDD[342] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 176 (MappedRDD[342] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 176.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 176.0:0 as TID 344 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 176.0:0 as 2396 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 176.0:1 as TID 345 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 176.0:1 as 2396 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 345
14/05/14 14:55:01 INFO executor.Executor: Running task ID 344
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 345 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 345 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 345
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 345 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(176, 1)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 344 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 344 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 344
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 344 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 176.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(176, 0)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 176 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009484 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 177 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 177 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 177 (MappedRDD[344] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 177 (MappedRDD[344] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 177.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 177.0:0 as TID 346 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 177.0:0 as 2386 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 177.0:1 as TID 347 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 177.0:1 as 2386 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 347
14/05/14 14:55:01 INFO executor.Executor: Running task ID 346
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 347 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 347 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 347
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 347 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(177, 1)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 346 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 346 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 346
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 346 in 7 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(177, 0)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 177.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 177 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.01113 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 178 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 178 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 178 (MappedRDD[346] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 178 (MappedRDD[346] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 178.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 178.0:0 as TID 348 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 178.0:0 as 2390 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 178.0:1 as TID 349 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 178.0:1 as 2390 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 348
14/05/14 14:55:01 INFO executor.Executor: Running task ID 349
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 349 is 945
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 348 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 348 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Sending result for 349 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 348
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 349
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 348 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(178, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 349 in 4 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(178, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 178.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 178 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008382 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 179 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 179 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 179 (MappedRDD[348] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 179 (MappedRDD[348] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 179.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 179.0:0 as TID 350 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 179.0:0 as 2384 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 179.0:1 as TID 351 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 179.0:1 as 2384 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 350
14/05/14 14:55:01 INFO executor.Executor: Running task ID 351
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 350 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 350 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 350
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 350 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(179, 0)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 351 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 351 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 351
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 351 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 179.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(179, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 179 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010425 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 180 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 180 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 180 (MappedRDD[350] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 180 (MappedRDD[350] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 180.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 180.0:0 as TID 352 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 180.0:0 as 2392 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 180.0:1 as TID 353 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 180.0:1 as 2392 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 352
14/05/14 14:55:01 INFO executor.Executor: Running task ID 353
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 352 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 352 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 352
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 352 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(180, 0)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 353 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 353 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 353
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 353 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 180.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(180, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 180 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009811 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 181 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 181 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 181 (MappedRDD[352] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 181 (MappedRDD[352] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 181.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 181.0:0 as TID 354 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 181.0:0 as 2385 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 181.0:1 as TID 355 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 181.0:1 as 2385 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 354
14/05/14 14:55:01 INFO executor.Executor: Running task ID 355
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 354 is 945
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 355 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 354 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Sending result for 355 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 354
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 355
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 354 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(181, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 355 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(181, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 181.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 181 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.012965 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 182 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 182 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 182 (MappedRDD[354] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 182 (MappedRDD[354] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 182.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 182.0:0 as TID 356 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 182.0:0 as 2392 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 182.0:1 as TID 357 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 182.0:1 as 2392 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 357
14/05/14 14:55:01 INFO executor.Executor: Running task ID 356
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 357 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 357 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 357
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 356 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 356 directly to driver
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 357 in 9 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(182, 1)
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 356
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 356 in 11 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 182.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(182, 0)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 182 (reduce at GradientDescent.scala:150) finished in 0.011 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.01552 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 183 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 183 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 183 (MappedRDD[356] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 183 (MappedRDD[356] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 183.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 183.0:0 as TID 358 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 183.0:0 as 2384 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 183.0:1 as TID 359 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 183.0:1 as 2384 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 358
14/05/14 14:55:01 INFO executor.Executor: Running task ID 359
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 358 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 358 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 358
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 359 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 359 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 359
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 358 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(183, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 359 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(183, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 183.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 183 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009626 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 184 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 184 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 184 (MappedRDD[358] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 184 (MappedRDD[358] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 184.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 184.0:0 as TID 360 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 184.0:0 as 2391 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 184.0:1 as TID 361 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 184.0:1 as 2391 bytes in 1 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 361
14/05/14 14:55:01 INFO executor.Executor: Running task ID 360
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 360 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 360 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 361 is 945
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 360
14/05/14 14:55:01 INFO executor.Executor: Sending result for 361 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 361
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 360 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(184, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 361 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(184, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 184.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 184 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010725 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 185 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 185 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 185 (MappedRDD[360] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 185 (MappedRDD[360] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 185.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 185.0:0 as TID 362 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 185.0:0 as 2436 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 185.0:1 as TID 363 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 185.0:1 as 2436 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 362
14/05/14 14:55:01 INFO executor.Executor: Running task ID 363
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 362 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 362 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 363 is 945
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 362
14/05/14 14:55:01 INFO executor.Executor: Sending result for 363 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 363
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 362 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(185, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 363 in 4 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(185, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 185.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 185 (reduce at GradientDescent.scala:150) finished in 0.004 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.007536 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 186 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 186 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 186 (MappedRDD[362] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 186 (MappedRDD[362] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 186.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 186.0:0 as TID 364 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 186.0:0 as 2402 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 186.0:1 as TID 365 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 186.0:1 as 2402 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 365
14/05/14 14:55:01 INFO executor.Executor: Running task ID 364
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 365 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 365 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 365
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 364 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 364 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 364
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 365 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(186, 1)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 364 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(186, 0)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 186.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 186 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009132 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 187 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 187 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 187 (MappedRDD[364] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 187 (MappedRDD[364] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 187.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 187.0:0 as TID 366 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 187.0:0 as 2408 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 187.0:1 as TID 367 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 187.0:1 as 2408 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 366
14/05/14 14:55:01 INFO executor.Executor: Running task ID 367
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 366 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 366 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 366
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 367 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 367 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 367
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 366 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(187, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 367 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(187, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 187.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 187 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.011919 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 188 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 188 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 188 (MappedRDD[366] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 188 (MappedRDD[366] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 188.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 188.0:0 as TID 368 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 188.0:0 as 2400 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 188.0:1 as TID 369 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 188.0:1 as 2400 bytes in 1 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 369
14/05/14 14:55:01 INFO executor.Executor: Running task ID 368
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 368 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 368 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 369 is 945
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 368
14/05/14 14:55:01 INFO executor.Executor: Sending result for 369 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 369
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 368 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(188, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 369 in 4 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(188, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 188.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 188 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008507 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 189 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 189 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 189 (MappedRDD[368] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 189 (MappedRDD[368] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 189.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 189.0:0 as TID 370 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 189.0:0 as 2394 bytes in 1 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 189.0:1 as TID 371 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 189.0:1 as 2394 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 370
14/05/14 14:55:01 INFO executor.Executor: Running task ID 371
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 370 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 370 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 370
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 371 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 371 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 371
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 370 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(189, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 371 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 189.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(189, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 189 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009955 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 190 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 190 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 190 (MappedRDD[370] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 190 (MappedRDD[370] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 190.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 190.0:0 as TID 372 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 190.0:0 as 2396 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 190.0:1 as TID 373 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 190.0:1 as 2396 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 373
14/05/14 14:55:01 INFO executor.Executor: Running task ID 372
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 373 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 373 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 373
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 373 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(190, 1)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 372 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 372 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 372
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 372 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(190, 0)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 190.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 190 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010483 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 191 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 191 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 191 (MappedRDD[372] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 191 (MappedRDD[372] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 191.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 191.0:0 as TID 374 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 191.0:0 as 2384 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 191.0:1 as TID 375 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 191.0:1 as 2384 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 374
14/05/14 14:55:01 INFO executor.Executor: Running task ID 375
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 374 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 374 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 374
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 374 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(191, 0)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 375 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 375 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 375
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 375 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(191, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 191.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 191 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008952 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 192 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 192 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 192 (MappedRDD[374] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 192 (MappedRDD[374] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 192.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 192.0:0 as TID 376 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 192.0:0 as 2386 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 192.0:1 as TID 377 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 192.0:1 as 2386 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 376
14/05/14 14:55:01 INFO executor.Executor: Running task ID 377
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 376 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 376 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 376
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 377 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 377 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 377
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 376 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(192, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 377 in 7 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 192.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(192, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 192 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.011085 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 193 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 193 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 193 (MappedRDD[376] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 193 (MappedRDD[376] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 193.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 193.0:0 as TID 378 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 193.0:0 as 2393 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 193.0:1 as TID 379 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 193.0:1 as 2393 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 378
14/05/14 14:55:01 INFO executor.Executor: Running task ID 379
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 379 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 379 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 378 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 378 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 379
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 378
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 379 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(193, 1)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 378 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(193, 0)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 193.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 193 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009689 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 194 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 194 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 194 (MappedRDD[378] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 194 (MappedRDD[378] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 194.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 194.0:0 as TID 380 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 194.0:0 as 2385 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 194.0:1 as TID 381 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 194.0:1 as 2385 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 380
14/05/14 14:55:01 INFO executor.Executor: Running task ID 381
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 380 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 380 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 380
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 380 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 381 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 381 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 381
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(194, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 381 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 194.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(194, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 194 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009513 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 195 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 195 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 195 (MappedRDD[380] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 195 (MappedRDD[380] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 195.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 195.0:0 as TID 382 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 195.0:0 as 2391 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 195.0:1 as TID 383 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 195.0:1 as 2391 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 382
14/05/14 14:55:01 INFO executor.Executor: Running task ID 383
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 382 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 382 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 382
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 383 is 945
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 382 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(195, 0)
14/05/14 14:55:01 INFO executor.Executor: Sending result for 383 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 383
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 383 in 4 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 195.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(195, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 195 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008328 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 196 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 196 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 196 (MappedRDD[382] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 196 (MappedRDD[382] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 196.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 196.0:0 as TID 384 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 196.0:0 as 2387 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 196.0:1 as TID 385 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 196.0:1 as 2387 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 384
14/05/14 14:55:01 INFO executor.Executor: Running task ID 385
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 384 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 384 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 384
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 384 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(196, 0)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 385 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 385 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 385
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 385 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 196.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(196, 1)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 196 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009874 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 197 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 197 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 197 (MappedRDD[384] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 197 (MappedRDD[384] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 197.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 197.0:0 as TID 386 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 197.0:0 as 2394 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 197.0:1 as TID 387 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 197.0:1 as 2394 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 386
14/05/14 14:55:01 INFO executor.Executor: Running task ID 387
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 386 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 386 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 386
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 386 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(197, 0)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 387 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 387 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 387
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 387 in 7 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(197, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 197.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 197 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.01184 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 198 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 198 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 198 (MappedRDD[386] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 198 (MappedRDD[386] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 198.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 198.0:0 as TID 388 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 198.0:0 as 2386 bytes in 1 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 198.0:1 as TID 389 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 198.0:1 as 2386 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 388
14/05/14 14:55:01 INFO executor.Executor: Running task ID 389
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 389 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 389 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 389
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 389 in 3 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(198, 1)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 388 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 388 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 388
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 388 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(198, 0)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 198.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 198 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008512 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 199 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 199 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 199 (MappedRDD[388] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 199 (MappedRDD[388] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 199.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 199.0:0 as TID 390 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 199.0:0 as 2391 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 199.0:1 as TID 391 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 199.0:1 as 2391 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 391
14/05/14 14:55:01 INFO executor.Executor: Running task ID 390
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 390 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 390 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 390
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 391 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 391 directly to driver
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 390 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(199, 0)
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 391
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 391 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(199, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 199.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 199 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009404 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 200 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 200 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 200 (MappedRDD[390] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 200 (MappedRDD[390] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 200.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 200.0:0 as TID 392 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 200.0:0 as 2434 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 200.0:1 as TID 393 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 200.0:1 as 2434 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 392
14/05/14 14:55:01 INFO executor.Executor: Running task ID 393
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 392 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 392 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 392
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 392 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(200, 0)
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 393 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 393 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 393
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 393 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(200, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 200.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 200 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010594 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 201 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 201 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 201 (MappedRDD[392] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 201 (MappedRDD[392] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 201.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 201.0:0 as TID 394 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 201.0:0 as 2401 bytes in 1 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 201.0:1 as TID 395 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 201.0:1 as 2401 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 395
14/05/14 14:55:01 INFO executor.Executor: Running task ID 394
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 395 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 395 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 395
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 394 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 394 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 394
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(201, 1)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 395 in 3 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 394 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(201, 0)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 201.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 201 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.007789 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 202 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 202 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 202 (MappedRDD[394] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 202 (MappedRDD[394] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 202.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 202.0:0 as TID 396 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 202.0:0 as 2410 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 202.0:1 as TID 397 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 202.0:1 as 2410 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 396
14/05/14 14:55:01 INFO executor.Executor: Running task ID 397
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 396 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 396 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 396
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 397 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 397 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 397
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(202, 0)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 396 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 397 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(202, 1)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 202.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 202 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.012659 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 203 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 203 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 203 (MappedRDD[396] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 203 (MappedRDD[396] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 203.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 203.0:0 as TID 398 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 203.0:0 as 2402 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 203.0:1 as TID 399 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 203.0:1 as 2402 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 398
14/05/14 14:55:01 INFO executor.Executor: Running task ID 399
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 399 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 399 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 398 is 945
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 399
14/05/14 14:55:01 INFO executor.Executor: Sending result for 398 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 398
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 399 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(203, 1)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Finished TID 398 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Completed ResultTask(203, 0)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 203.0, whose tasks have all completed, from pool 
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Stage 203 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:01 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009921 s
14/05/14 14:55:01 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Got job 204 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Final stage: Stage 204 (reduce at GradientDescent.scala:150)
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting Stage 204 (MappedRDD[398] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:01 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 204 (MappedRDD[398] at map at GradientDescent.scala:145)
14/05/14 14:55:01 INFO scheduler.TaskSchedulerImpl: Adding task set 204.0 with 2 tasks
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 204.0:0 as TID 400 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 204.0:0 as 2398 bytes in 0 ms
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Starting task 204.0:1 as TID 401 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:01 INFO scheduler.TaskSetManager: Serialized task 204.0:1 as 2398 bytes in 0 ms
14/05/14 14:55:01 INFO executor.Executor: Running task ID 400
14/05/14 14:55:01 INFO executor.Executor: Running task ID 401
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:01 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 400 is 945
14/05/14 14:55:01 INFO executor.Executor: Serialized size of result for 401 is 945
14/05/14 14:55:01 INFO executor.Executor: Sending result for 400 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Sending result for 401 directly to driver
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 401
14/05/14 14:55:01 INFO executor.Executor: Finished task ID 400
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 400 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(204, 0)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 401 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 204.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(204, 1)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 204 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.011601 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 205 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 205 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 205 (MappedRDD[400] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 205 (MappedRDD[400] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 205.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 205.0:0 as TID 402 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 205.0:0 as 2394 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 205.0:1 as TID 403 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 205.0:1 as 2394 bytes in 1 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 402
14/05/14 14:55:02 INFO executor.Executor: Running task ID 403
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 402 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 402 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 402
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 402 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(205, 0)
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 403 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 403 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 403
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 403 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 205.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(205, 1)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 205 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009482 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 206 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 206 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 206 (MappedRDD[402] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 206 (MappedRDD[402] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 206.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 206.0:0 as TID 404 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 206.0:0 as 2387 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 206.0:1 as TID 405 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 206.0:1 as 2387 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 404
14/05/14 14:55:02 INFO executor.Executor: Running task ID 405
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 405 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 405 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 404 is 945
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 405
14/05/14 14:55:02 INFO executor.Executor: Sending result for 404 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 404
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 405 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(206, 1)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 404 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(206, 0)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 206.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 206 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008367 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 207 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 207 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 207 (MappedRDD[404] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 207 (MappedRDD[404] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 207.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 207.0:0 as TID 406 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 207.0:0 as 2388 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 207.0:1 as TID 407 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 207.0:1 as 2388 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 406
14/05/14 14:55:02 INFO executor.Executor: Running task ID 407
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 407 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 407 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 406 is 945
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 407
14/05/14 14:55:02 INFO executor.Executor: Sending result for 406 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 406
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 407 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(207, 1)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 406 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(207, 0)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 207.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 207 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.00981 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 208 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 208 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 208 (MappedRDD[406] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 208 (MappedRDD[406] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 208.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 208.0:0 as TID 408 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 208.0:0 as 2395 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 208.0:1 as TID 409 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 208.0:1 as 2395 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 408
14/05/14 14:55:02 INFO executor.Executor: Running task ID 409
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 408 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 408 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 409 is 945
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 408
14/05/14 14:55:02 INFO executor.Executor: Sending result for 409 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 409
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 408 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(208, 0)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 409 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(208, 1)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 208.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 208 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009227 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 209 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 209 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 209 (MappedRDD[408] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 209 (MappedRDD[408] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 209.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 209.0:0 as TID 410 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 209.0:0 as 2386 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 209.0:1 as TID 411 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 209.0:1 as 2386 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 410
14/05/14 14:55:02 INFO executor.Executor: Running task ID 411
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 411 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 411 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 411
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 410 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 410 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 410
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 411 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(209, 1)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 410 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(209, 0)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 209.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 209 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009834 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 210 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 210 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 210 (MappedRDD[410] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 210 (MappedRDD[410] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 210.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 210.0:0 as TID 412 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 210.0:0 as 2390 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 210.0:1 as TID 413 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 210.0:1 as 2390 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 412
14/05/14 14:55:02 INFO executor.Executor: Running task ID 413
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 413 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 413 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 413
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 413 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 412 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 412 directly to driver
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(210, 1)
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 412
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(210, 0)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 412 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 210.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 210 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008806 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 211 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 211 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 211 (MappedRDD[412] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 211 (MappedRDD[412] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 211.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 211.0:0 as TID 414 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 211.0:0 as 2387 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 211.0:1 as TID 415 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 211.0:1 as 2387 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 414
14/05/14 14:55:02 INFO executor.Executor: Running task ID 415
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 414 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 414 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 414
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 415 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 415 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 415
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(211, 0)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 414 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 415 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(211, 1)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 211.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 211 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009116 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 212 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 212 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 212 (MappedRDD[414] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 212 (MappedRDD[414] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 212.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 212.0:0 as TID 416 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 212.0:0 as 2391 bytes in 1 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 212.0:1 as TID 417 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 212.0:1 as 2391 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 416
14/05/14 14:55:02 INFO executor.Executor: Running task ID 417
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 416 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 416 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 416
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 417 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 417 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 417
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 416 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(212, 0)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 417 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 212.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(212, 1)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 212 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010223 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 213 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 213 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 213 (MappedRDD[416] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 213 (MappedRDD[416] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 213.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 213.0:0 as TID 418 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 213.0:0 as 2385 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 213.0:1 as TID 419 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 213.0:1 as 2385 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 418
14/05/14 14:55:02 INFO executor.Executor: Running task ID 419
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 418 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 418 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 418
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 419 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 419 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 419
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 418 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(213, 0)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 419 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(213, 1)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 213.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 213 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010763 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 214 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 214 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 214 (MappedRDD[418] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 214 (MappedRDD[418] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 214.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 214.0:0 as TID 420 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 214.0:0 as 2438 bytes in 1 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 214.0:1 as TID 421 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 214.0:1 as 2438 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 420
14/05/14 14:55:02 INFO executor.Executor: Running task ID 421
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 420 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 420 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 420
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 420 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(214, 0)
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 421 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 421 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 421
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 421 in 4 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(214, 1)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 214.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 214 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008404 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 215 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 215 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 215 (MappedRDD[420] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 215 (MappedRDD[420] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 215.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 215.0:0 as TID 422 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 215.0:0 as 2400 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 215.0:1 as TID 423 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 215.0:1 as 2400 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 422
14/05/14 14:55:02 INFO executor.Executor: Running task ID 423
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 423 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 423 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 423
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 422 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 422 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 422
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(215, 1)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 423 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 422 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 215.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(215, 0)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 215 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009252 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 216 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 216 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 216 (MappedRDD[422] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 216 (MappedRDD[422] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 216.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 216.0:0 as TID 424 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 216.0:0 as 2401 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 216.0:1 as TID 425 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 216.0:1 as 2401 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 425
14/05/14 14:55:02 INFO executor.Executor: Running task ID 424
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 425 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 425 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 425
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 424 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 424 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 424
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 425 in 8 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(216, 1)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(216, 0)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 424 in 9 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 216.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 216 (reduce at GradientDescent.scala:150) finished in 0.009 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.012722 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 217 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 217 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 217 (MappedRDD[424] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 217 (MappedRDD[424] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 217.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 217.0:0 as TID 426 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 217.0:0 as 2396 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 217.0:1 as TID 427 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 217.0:1 as 2396 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 426
14/05/14 14:55:02 INFO executor.Executor: Running task ID 427
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 427 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 427 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 426 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 426 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 427
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 426
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 427 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(217, 1)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(217, 0)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 426 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 217.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 217 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008997 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 218 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 218 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 218 (MappedRDD[426] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 218 (MappedRDD[426] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 218.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 218.0:0 as TID 428 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 218.0:0 as 2395 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 218.0:1 as TID 429 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 218.0:1 as 2395 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 428
14/05/14 14:55:02 INFO executor.Executor: Running task ID 429
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 428 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 428 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 429 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 429 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 428
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 429
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 428 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(218, 0)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 429 in 4 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(218, 1)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 218.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 218 (reduce at GradientDescent.scala:150) finished in 0.004 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.00719 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 219 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 219 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 219 (MappedRDD[428] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 219 (MappedRDD[428] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 219.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 219.0:0 as TID 430 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 219.0:0 as 2394 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 219.0:1 as TID 431 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 219.0:1 as 2394 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 430
14/05/14 14:55:02 INFO executor.Executor: Running task ID 431
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 431 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 431 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 431
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 430 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 430 directly to driver
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 431 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(219, 1)
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 430
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 430 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(219, 0)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 219.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 219 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.009575 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 220 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 220 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 220 (MappedRDD[430] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 220 (MappedRDD[430] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 220.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 220.0:0 as TID 432 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 220.0:0 as 2385 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 220.0:1 as TID 433 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 220.0:1 as 2385 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 432
14/05/14 14:55:02 INFO executor.Executor: Running task ID 433
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 433 is 945
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 432 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 433 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Sending result for 432 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 432
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 433
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 433 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(220, 1)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 432 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 220.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(220, 0)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 220 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.01079 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 221 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 221 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 221 (MappedRDD[432] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 221 (MappedRDD[432] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 221.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 221.0:0 as TID 434 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 221.0:0 as 2391 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 221.0:1 as TID 435 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 221.0:1 as 2391 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 434
14/05/14 14:55:02 INFO executor.Executor: Running task ID 435
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 435 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 435 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 434 is 945
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 435
14/05/14 14:55:02 INFO executor.Executor: Sending result for 434 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 434
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 435 in 6 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(221, 1)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 434 in 6 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(221, 0)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 221.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 221 (reduce at GradientDescent.scala:150) finished in 0.007 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010173 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 222 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 222 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 222 (MappedRDD[434] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 222 (MappedRDD[434] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 222.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 222.0:0 as TID 436 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 222.0:0 as 2384 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 222.0:1 as TID 437 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 222.0:1 as 2384 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 436
14/05/14 14:55:02 INFO executor.Executor: Running task ID 437
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 437 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 437 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 437
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 436 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 436 directly to driver
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 437 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 436
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(222, 1)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(222, 0)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 436 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 222.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 222 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008062 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 223 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 223 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 223 (MappedRDD[436] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 223 (MappedRDD[436] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 223.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 223.0:0 as TID 438 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 223.0:0 as 2393 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 223.0:1 as TID 439 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 223.0:1 as 2393 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 438
14/05/14 14:55:02 INFO executor.Executor: Running task ID 439
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 439 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 439 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 439
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 439 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(223, 1)
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 438 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 438 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 438
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 438 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(223, 0)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 223.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 223 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010314 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 224 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 224 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 224 (MappedRDD[438] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 224 (MappedRDD[438] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 224.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 224.0:0 as TID 440 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 224.0:0 as 2385 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 224.0:1 as TID 441 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 224.0:1 as 2385 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 440
14/05/14 14:55:02 INFO executor.Executor: Running task ID 441
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 441 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 441 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 441
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 440 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 440 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 440
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 441 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(224, 1)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 440 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(224, 0)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 224.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 224 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008246 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 225 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 225 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 225 (MappedRDD[440] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 225 (MappedRDD[440] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 225.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 225.0:0 as TID 442 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 225.0:0 as 2391 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 225.0:1 as TID 443 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 225.0:1 as 2391 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 442
14/05/14 14:55:02 INFO executor.Executor: Running task ID 443
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 442 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 442 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 442
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 443 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 443 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 443
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 442 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(225, 0)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(225, 1)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 443 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 225.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 225 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008273 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 226 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 226 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 226 (MappedRDD[442] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 226 (MappedRDD[442] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 226.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 226.0:0 as TID 444 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 226.0:0 as 2387 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 226.0:1 as TID 445 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 226.0:1 as 2387 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 444
14/05/14 14:55:02 INFO executor.Executor: Running task ID 445
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 444 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 444 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 444
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 445 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 445 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 445
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(226, 0)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 444 in 7 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 445 in 8 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 226.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(226, 1)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 226 (reduce at GradientDescent.scala:150) finished in 0.008 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.0122 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 227 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 227 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 227 (MappedRDD[444] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 227 (MappedRDD[444] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 227.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 227.0:0 as TID 446 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 227.0:0 as 2392 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 227.0:1 as TID 447 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 227.0:1 as 2392 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 446
14/05/14 14:55:02 INFO executor.Executor: Running task ID 447
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 446 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 446 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 446
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 446 in 3 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(227, 0)
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 447 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 447 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 447
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 447 in 4 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(227, 1)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 227.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 227 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.008364 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 228 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 228 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 228 (MappedRDD[446] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 228 (MappedRDD[446] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 228.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 228.0:0 as TID 448 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 228.0:0 as 2386 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 228.0:1 as TID 449 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 228.0:1 as 2386 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 448
14/05/14 14:55:02 INFO executor.Executor: Running task ID 449
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 448 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 448 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 448
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 448 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(228, 0)
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 449 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 449 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 449
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 449 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 228.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(228, 1)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 228 (reduce at GradientDescent.scala:150) finished in 0.006 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.010441 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: reduce at GradientDescent.scala:150
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 229 (reduce at GradientDescent.scala:150) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 229 (reduce at GradientDescent.scala:150)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 229 (MappedRDD[448] at map at GradientDescent.scala:145), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 229 (MappedRDD[448] at map at GradientDescent.scala:145)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 229.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 229.0:0 as TID 450 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 229.0:0 as 2433 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 229.0:1 as TID 451 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 229.0:1 as 2433 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 451
14/05/14 14:55:02 INFO executor.Executor: Running task ID 450
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 451 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 451 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 451
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 450 is 945
14/05/14 14:55:02 INFO executor.Executor: Sending result for 450 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 450
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 451 in 4 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(229, 1)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 450 in 5 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(229, 0)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 229.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 229 (reduce at GradientDescent.scala:150) finished in 0.005 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: reduce at GradientDescent.scala:150, took 0.007658 s
14/05/14 14:55:02 INFO optimization.GradientDescent: GradientDescent finished. Last 10 stochastic losses 0.9619247843959207, 0.9621003977214416, 0.9617559501745715, 0.961893230281189, 0.9616126809315697, 0.9616198111985078, 0.9614780401892526, 0.9614126195709279, 0.9615367570107078, 0.970199902845025
14/05/14 14:55:02 INFO classification.SVMWithSGD: Final weights -0.015415502759873329,0.0,0.0,0.34449165180412294,-0.0,-0.0,0.0,-0.46716942217845064,0.0,0.0,0.001964011217187038,-0.0021188716335119014,0.0,-0.0,0.0,0.0
14/05/14 14:55:02 INFO classification.SVMWithSGD: Final intercept 0.0
modelL1: org.apache.spark.mllib.classification.SVMModel@f401c44
evaluateModel - Second
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: count at BinaryClassification_tut1.scala:45
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 230 (count at BinaryClassification_tut1.scala:45) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 230 (count at BinaryClassification_tut1.scala:45)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 230 (FilteredRDD[450] at filter at BinaryClassification_tut1.scala:45), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 230 (FilteredRDD[450] at filter at BinaryClassification_tut1.scala:45)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 230.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 230.0:0 as TID 452 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 230.0:0 as 2052 bytes in 1 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 230.0:1 as TID 453 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 230.0:1 as 2052 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 452
14/05/14 14:55:02 INFO executor.Executor: Running task ID 453
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 453 is 563
14/05/14 14:55:02 INFO executor.Executor: Sending result for 453 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 453
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 453 in 5 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(230, 1)
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 452 is 563
14/05/14 14:55:02 INFO executor.Executor: Sending result for 452 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 452
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 452 in 7 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(230, 0)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 230.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 230 (count at BinaryClassification_tut1.scala:45) finished in 0.007 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: count at BinaryClassification_tut1.scala:45, took 0.008939 s
14/05/14 14:55:02 INFO spark.SparkContext: Starting job: count at BinaryClassification_tut1.scala:45
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Got job 231 (count at BinaryClassification_tut1.scala:45) with 2 output partitions (allowLocal=false)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Final stage: Stage 231 (count at BinaryClassification_tut1.scala:45)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Parents of final stage: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Missing parents: List()
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting Stage 231 (MappedRDD[2] at map at BinaryClassification_tut1.scala:19), which has no missing parents
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 231 (MappedRDD[2] at map at BinaryClassification_tut1.scala:19)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Adding task set 231.0 with 2 tasks
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 231.0:0 as TID 454 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 231.0:0 as 1704 bytes in 0 ms
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Starting task 231.0:1 as TID 455 on executor localhost: localhost (PROCESS_LOCAL)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Serialized task 231.0:1 as 1704 bytes in 0 ms
14/05/14 14:55:02 INFO executor.Executor: Running task ID 454
14/05/14 14:55:02 INFO executor.Executor: Running task ID 455
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO storage.BlockManager: Found block broadcast_0 locally
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:19737+19737
14/05/14 14:55:02 INFO rdd.HadoopRDD: Input split: file:/Users/hieronymus/Development/Workspace/spark_sample/test-data/sample_svm_data.txt:0+19737
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 454 is 563
14/05/14 14:55:02 INFO executor.Executor: Serialized size of result for 455 is 563
14/05/14 14:55:02 INFO executor.Executor: Sending result for 454 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Sending result for 455 directly to driver
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 455
14/05/14 14:55:02 INFO executor.Executor: Finished task ID 454
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 454 in 8 ms on localhost (progress: 1/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(231, 0)
14/05/14 14:55:02 INFO scheduler.TaskSetManager: Finished TID 455 in 9 ms on localhost (progress: 2/2)
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Completed ResultTask(231, 1)
14/05/14 14:55:02 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 231.0, whose tasks have all completed, from pool 
14/05/14 14:55:02 INFO scheduler.DAGScheduler: Stage 231 (count at BinaryClassification_tut1.scala:45) finished in 0.009 s
14/05/14 14:55:02 INFO spark.SparkContext: Job finished: count at BinaryClassification_tut1.scala:45, took 0.013552 s
training Error for Second model : 0.3944099378881988
